Topic,Count,Name,Representation,Representative_Docs
-1,112,-1_bias_data_lead_groups,"['bias', 'data', 'lead', 'groups', 'examples', 'adversarial', 'models', 'systematic', 'development', 'attributions']","['an example of biased decision system that can nonetheless be considered fair is university hiring policy that is biased in favour of people with relevant qualifications in that it hires far greater proportion of holders of relevant qualifications than the proportion of relevant qualification holders in the population', '3.2.5 confirmation bias type of human cognitive bias 3.2.4 that favours predictions of ai systems that confirm pre-existing beliefs or hypotheses 3.2.6 convenience sample sample of data that is chosen because it is easy to obtain rather than because it is representative 3.2.7 data bias data properties that if unaddressed lead to ai systems that perform better or worse for different groups 3.2.8 \ufeff\x08 3.2.8 group subset of objects in domain that are linked because they have shared characteristics 3.2.10 statistical bias type of consistent numerical offset in an estimate relative to the true underlying value inherent to most estimates source iso 20501:2019 3.3.9 abbreviations ai artificial intelligence ml machine learning overview of bias and fairness 5.1 general in this document the term bias is defined as systematic difference in the treatment of certain objects people or groups in comparison to others in its generic meaning beyond the context of ai or ml', 'for example any systematic over-arrest and conviction of particular group of people would then lead to the systematic over-classification of recidivism among this population of prisoners']"
0,40,0_training_validation_datasets_feature,"['training', 'validation', 'datasets', 'feature', 'sources', 'bias', 'evaluators', 'omission', 'clustering', 'methods']","['it is important to consider how the data relates to the purpose of building the system the process by which features are chosen and the individuals that are choosing features and their rationale including any associated explicitly identified assumptions .it is important to evaluate the chosen features for any data and human cognitive biases such as missing feature values unexpected feature values or data skew', 'hold-out dataset obtained from data source independent of the training dataset is typically used in verification and validation', 'data-based methods can be used to mitigate bias in the training data']"
1,37,1_cognitive_bias_decisions_automation,"['cognitive', 'bias', 'decisions', 'automation', 'information', 'implicit', 'consent', 'association', 'tracking', 'judgement']","['this can be caused by the human cognitive biases described in 6.2', '3.1.5 user individual or group that interacts with system or benefits from system during its utilization source iso/iec/ieee 15288:2015 4.1.52 3.2 bias 3.2.1 automation bias propensity for humans to favour suggestions from automated decision-making systems and to ignore contradictory information made without automation even if it is correct 3.2.2 bias systematic difference in treatment of certain objects people or groups in comparison to others note to entry treatment is any kind of action including perception observation representation prediction or decision 3.2.4 human cognitive bias bias 3.2.2 that occurs when humans are processing and interpreting information note to entry human cognitive bias influences judgement and decision-making', 'the human cognitive biases 6.2 can cause bias to be introduced through engineering decisions 6.4 or data bias 6.3']"
2,32,2_stakeholders_requirements_inclusiveness_concerns,"['stakeholders', 'requirements', 'inclusiveness', 'concerns', 'stakeholder', 'regulatory', 'governance', 'international', 'obligations', 'engagement']","['\ufeff\x08 8.2.3 internal requirements in addition to regulatory requirements many other factors can contribute to stakeholder desire to mitigate bias such as internal goals strategies and policies of an organization moral or cultural values avoiding societal concerns or reputational damage', 'the analysis process can give special regard to five specific areas the inclusion of trans-disciplinary experts the identification of stakeholders the selection of data sources external change and specification of acceptance criteria including acceptable levels of bias', '8.2.5 identification of stakeholders traditional requirements analysis includes the identification of stakeholders']"
3,31,3_bias_societal_automation_outcomes,"['bias', 'societal', 'automation', 'outcomes', 'specifying', 'describes', 'target', 'research', 'developers', 'objectives']","['for example an ai system used for hiring specific type of worker can introduce bias towards one gender over another in the decision phase to compensate for societal bias inherited from the data which reflects their historical underrepresentation in this profession', '\ufeff\x08 vi information technology artificial intelligence ai bias in ai systems and ai aided decision making scope this document addresses bias in relation to ai systems especially with regards to ai-aided decision- making', '6.2.2 automation bias ai assists automation of analysis and decision-making in various systems for example in self-driving cars and health-care systems that can invite automation bias']"
4,26,4_fairness_metrics_demographics_discriminate,"['fairness', 'metrics', 'demographics', 'discriminate', 'qualifications', 'social', 'groups', 'unfair', 'employment', 'favouritism']","['prior to testing fairness objectives can be made explicit this includes determination of relevant demographic characteristics selection and justification of fairness metrics to be used in detecting bias and fixing the allowable margin of difference delta', 'such metrics are described in the literature on algorithmic fairness and are referred to as fairness metrics or metrics of algorithmic fairness', '7.7 other metrics alternative metrics can include minimax fairness and pareto fairness']"
5,26,5_false_demographic_parity_condition,"['false', 'demographic', 'parity', 'condition', 'categories', 'positives', 'loan', 'applicant', 'fnr', 'table']","['\ufeff\x08 this implies equal true positive rates tpr across demographic categories', 'table a.1 confusion matrix for immigrant applications in example true condition total popula- tion condition positive condition negative total prediction predicted condition prediction positive prediction neg- ative total condition tpr 0,98 tnr 1,00 acc 0,98 fpr 0,00 fnr 0,02 \ufeff\x08 table a.2 confusion matrix for non-immigrant applications in example true condition total popula- tion condition positive condition negative total prediction predicted con- dition prediction positive prediction negative total condi- tion tpr 0,99 tnr 0,90 acc 0,98 fpr 0,10 fnr 0,01 since the rejection rate for immigrants lies within of the rejection rate for non-immigrants demographic parity holds', 'this implies that true positive rates tpr are equal across demographic categories and false positive rates fpr are equal across demographic categories']"
6,25,6_models_expressiveness_explainability_ml,"['models', 'expressiveness', 'explainability', 'ml', 'classification', 'predicate', 'inference', 'informativeness', 'python', 'language']","['there can be many sub-models inside an ml model that can be interacting with linear combination or more complex combination of the sub-models', 'for example in an ml model for natural language question answering system there can be combination of predicate-prediction model value-identification model predicate-value binding model and constraints-identification model', 'an open source python library that helps to understand the learned structures of black box model both globally inference based on complete dataset and locally inference about an individual prediction']"
7,25,7_sampling_population_samples_coverage,"['sampling', 'population', 'samples', 'coverage', 'dataset', 'selection', 'bias', 'lognormal', 'simpson', 'gamma']","['reduce data bias non-representative sampling', 'non-representative sampling is an example of biased training data selection', '6.3.4 non-representative sampling bias can manifest in several ways during training data selection as result of the human cognitive biases described in 6.2 or due to sampling or coverage bias as described in 6.3.2']"
8,25,8_accuracy_data_outliers_defects,"['accuracy', 'data', 'outliers', 'defects', 'records', 'signals', 'lineage', 'sensors', 'resolution', 'formalized']","['the frequency that the data records are collected or updated can be relevant to ensure their accuracy', 'it is important to understand the lineage of data how it is collected how it is input and whether these processes affect completeness and accuracy', 'source iso 16269-4:2010 2.1 modified added statistics domain 3.1.4 knowledge information about objects events concepts or rules their relationships and properties organized for goal-oriented systematic use note to entry information can exist in numeric or symbolic form']"
9,21,9_labels_labelling_labellers_data,"['labels', 'labelling', 'labellers', 'data', 'cognitive', 'annotate', 'annotation', 'tasks', 'label', 'sample']","['during data labelling it is possible for the human cognitive bias of the data labellers to be introduced into the data', '6.3.3 data labels and labelling process the labelling process itself potentially introduces the cognitive or societal biases described in 6.2 to the data', '8.4.3 sample checks of labels the risk of incorrect labelling described in 6.3.3 that is human labellers incorrectly specifying the labels for set of input data then used to train the model can be assessed through sample checks of submitted labels']"
10,20,10_gender_arrests_resume_candidates,"['gender', 'arrests', 'resume', 'candidates', 'immigrant', 'issues', 'algorithm', 'features', 'misinterpreted', 'ms']","['while removing ethnicity gender and age can appear to address the issue other features acting as proxy variables can indirectly reflect bias', 'other examples of proxy variables include music tastes and age shopping patterns and gender zip codes and race and income levels family status and gender education which university or college person graduated from and race weight or height and gender etc', 'for instance in use case that automatically short-lists candidates based on resume information examples of such features can be ethnicity gender and age']"
11,19,11_homogeneity_groups_perceived_variability,"['homogeneity', 'groups', 'perceived', 'variability', 'traits', 'genomics', 'diversity', 'research', 'ethnic', 'imbalance']","[""the out-group homogeneity effect is an individual perception of out-group members as more similar to one another than are in-group members for example `` they are alike we are diverse ''"", '6.2.7 out-group homogeneity bias out-group homogeneity bias occurs when seeing out-group members as more alike than in-group members when comparing attitudes values personality traits and other characteristics', ""the term `` out- group homogeneity effect '' or `` relative out-group homogeneity has been explicitly contrasted with out-group homogeneity in general the latter referring to perceived out-group variability unrelated to perceptions of the in-group""]"
12,19,12_people_recognition_facial_tone,"['people', 'recognition', 'facial', 'tone', 'skin', 'average', 'dataset', 'group', 'categories', 'features']","['\ufeff\x08 for example the ai developer can choose to represent height of people through categorial values such as tall average or short and then choose the ranges in such way that majority of one gender falls in the average and short category while the majority of another falls in the tall and average category', 'the number of images of people with particular skin tone the lighting conditions of images and the relative entropy of images of people with one skin tone are examples of how non-representative dataset can introduce bias into model', 'for example in the domain of facial recognition there are several different ways for dataset to be non-representative with respect to attributes such as skin tone']"
13,18,13_fairness_unfair_unfairness_assessing,"['fairness', 'unfair', 'unfairness', 'assessing', 'parties', 'opportunities', 'mitigation', 'outlier', 'fairml', 'fairlearn']","['these components are designed to assess and improve the fairness of ai systems while navigating trade-offs between fairness and model performance', ""fairtest an open source tool that enables developers or auditing entities to discover and test for unwarranted associations between an algorithm 's outputs and certain user subpopulations identified by protected features. themis testing approach and tool for measuring discrimination in software system"", 'common categories of negative impacts that can be perceived as unfair include unfair allocation occurs when an ai system unfairly extends or withholds opportunities or resources in ways that have negative effects on some parties as compared to others']"
14,17,14_amplify_activation_likelihood_learn,"['amplify', 'activation', 'likelihood', 'learn', 'estimator', 'outputs', 'bias', 'calibrate', 'generalization', 'downstream']","['ai systems often learn from real-world data hence an ml model can learn or even amplify problematic pre-existing data bias', 'downstream activation functions like the sigmoid function can amplify small differences in features that are the result of data bias', '6.4.6 model bias given that ml often uses functions like maximum likelihood estimator to determine parameters if there is data skew or under-representation present in the data the maximum likelihood estimation tends to amplify any underlying bias in the distribution']"
15,16,15_ensemble_prediction_forest_threshold,"['ensemble', 'prediction', 'forest', 'threshold', 'set', 'models', 'randomized', 'probabilities', 'boosting', 'averaging']","['random forest is an approach that combines several randomized decision trees and aggregates their predictions by averaging', 'for example ml model can use sequential construction of shallow regression trees to form an ensemble and give prediction as sum of the trees prediction probabilities', 'however the ensemble of such sub-models can introduce unwanted bias in the final predictions']"
16,16,16_societal_historical_bias_cultural,"['societal', 'historical', 'bias', 'cultural', 'norms', 'smoking', 'weightlifters', 'sumo', 'accommodate', 'wrestlers']","['figure relationship between high-level groups of bias for example written or spoken language contains societal bias which can be amplified by word embedding models', '6.2.8 societal bias societal bias occurs when similar cognitive bias conscious or unconscious is being held by many individuals in society', 'historical and societal biases']"
17,16,17_testing_validity_external_testers,"['testing', 'validity', 'external', 'testers', 'pool', 'tests', 'diverse', 'users', 'evaluates', 'empirical']","['8.4.4 internal validity testing internal validity testing evaluates the correlation between individual input data items and the system outputs', 'external validity testing can also include integrating new input data and validating that the results are consistent with internal validity testing', '8.4.5 external validity testing external validity testing can involve re-evaluating prior observations using external data sources']"
18,16,18_feature_features_selection_regression,"['feature', 'features', 'selection', 'regression', 'ridge', 'lasso', 'regularization', 'relevance', 'parameters', 'iofp']","['using series of steps it is possible to find out the feature contribution and the relative significance of each feature in model prediction', 'note that although these approaches have been used to perform feature selection or feature relevance they are not always directly applicable to determining biases present within data', '6.4.2 feature engineering during the feature engineering process in building an ml model the ai developers can directly use any of the input features or can create complex features for the ml model from input features in such way that they can be linear or non-linear combinations of some of the input features']"
19,16,19_model_hyperparameters_inputs_ranking,"['model', 'hyperparameters', 'inputs', 'ranking', 'architecture', 'learning', 'layer', 'tuning', 'optimal', 'neurons']","['it uses four input ranking algorithms to quantify model relative predictive dependence on model inputs', 'hyperparameters include the number of network layers the number of neurons in layer also called the width of each layer the learning rate for gradient descent the degree of polynomials to use for the linear model and the number of trees in random forest etc', ""given the input and output to an ml model the method seeks to produce an input ranking that corresponds to the machine learning system 's dependence on each input in its decision-making process and thus can detect bias involving certain features""]"
20,15,20_transparency_template_models_documentation,"['transparency', 'template', 'models', 'documentation', 'technical', 'tools', 'integrity', 'factsheets', 'projects', 'academic']","['transparency projects dealing with ai-systems can have different understanding of what transparency mean', 'the usefulness and accuracy of transparency tool relies on the integrity of the creator of the tool itself and can be stored as documentation or meta-data associated with each model', '8.5.3 transparency tools to clarify the intended use cases of ml models and minimize their usage in contexts for which they are not well suited released models can be accompanied by documentation detailing their performance characteristics']"
21,14,21_strategies_objective_risk_ethics,"['strategies', 'objective', 'risk', 'ethics', 'organization', 'governance', 'ethical', 'bias', 'treatments', 'mitigations']","['c.6 bias treatment strategy examples table c.5 shows examples of bias treatment strategies from this document that can be applied', '\ufeff\x08 table c.2 organization governance level objective risk source bias treatment strategies fairness ai systems can be used against law or unethically', 'table c.4 ai project level objective risk source bias treatment strategies fairness ethical dilemmas']"
22,14,22_risks_publication_confidential_stage,"['risks', 'publication', 'confidential', 'stage', 'layers', 'products', 'organically', 'markets', 'timeliness', 'meta']","['the examples show that as the risk management propagates through the layers the identified controls of an upper layer often become the objectives of the layer or layers', 'stage at the time of publication iso/dis 22989:2021', 'stage at the time of publication iso/dis 23053:2021']"
23,14,23_knowledge_input_predicting_helsinki,"['knowledge', 'input', 'predicting', 'helsinki', 'registers', 'jobs', 'matching', 'varied', 'cities', 'recommending']","['an ai system can be characterized as using knowledge to process input data to make predictions or take actions', 'however for classification systems it is useful to think of the ai predictions as processing the set of input data presented to it and predicting that the input belongs to desired set or not', 'ai systems can be used for various tasks such as recommending books and television shows predicting the presence and severity of medical condition matching people to jobs and partners or identifying if person is crossing the street']"
24,14,24_deployment_technology_preparation_developed,"['deployment', 'technology', 'preparation', 'developed', 'sc', 'parameter', 'management', 'evaluation', 'estimator', 'standardization']","['normative references iso/iec information technology artificial intelligence artificial intelligence concepts and terminology iso/iec framework for artificial intelligence ai systems using machine learning ml terms and definitions for the purposes of this document the following terms and definitions given in iso/iec and iso/ iec and the following apply', 'these are being developed by sc and include iso/iec iso/iec information technology artificial intelligence management system in preparation iso/iec information technology artificial intelligence risk management in preparation', 'iso and iec maintain terminological databases for use in standardization at the following addresses iso online browsing platform available at iec electropedia available at 3.1 artificial intelligence 3.1.1 maximum likelihood estimator estimator assigning the value of the parameter where the likelihood function attains or approaches its highest value note to entry maximum likelihood estimation is well-established approach for obtaining parameter estimates where distribution has been specified for example normal gamma weibull and so forth']"
25,13,25_applications_rejects_exclusion_scenario,"['applications', 'rejects', 'exclusion', 'scenario', 'rejected', 'stakeholder', 'business', 'loan', 'systemic', 'immigration']","['in first version the ai system in operation and trained on all previous loan applications rejected of immigrant applications but only of non-immigrant applications', 'desirable ai system would correctly predict whether the application represents an acceptable risk without contributing to systemic exclusion of certain groups', 'desirable ai system would correctly predict whether the application represents an acceptable risk without contributing to systemic exclusion of certain groups']"
26,12,26_impact_bias_preference_obstacle,"['impact', 'bias', 'preference', 'obstacle', 'values', 'vulnerabilities', 'analysing', 'seek', 'metrics', 'offset']","['however investigation would be necessary to assess whether the impact of this bias is positive neutral or negative according to the system goals and objectives', 'however this statistical bias will have neutral impact as long as the system has an equally strong preference for avoiding each type of obstacle', 'bias can have negative positive or neutral impact']"
27,12,27_mitigation_algorithms_technique_features,"['mitigation', 'algorithms', 'technique', 'features', 'edits', 'indications', 'retraining', 'unsophisticated', 'categorisation', 'consequence']","['seemingly straightforward approach to bias mitigation is to remove the relevant features that can be responsible for the unwanted bias directly', 'thus removing only some of the obvious features that are associated with unwanted bias does not always result in bias mitigation', 'examples of bias mitigation algorithms that are applied are disparate impact remover pre-processing technique that edits values that will be used as features in such way to reduce different treatment between the groups']"
28,11,28_values_equalized_odds_opportunity,"['values', 'equalized', 'odds', 'opportunity', 'conditional', 'tnr', 'popula', 'fnr', 'classifier', 'predicted']","['7.2 confusion matrix confusion matrix see figure is tool that can be used to evaluate the performance of classifier', 'figure confusion matrix and derived classification performance metrics 7.3 equalized odds equalized odds means that an algorithm decisions are independent of category given the input', 'table a.3 confusion matrix for immigrant applications in example true condition total popula- tion condition positive condition negative total prediction predicted condition prediction positive prediction neg- ative total condition tpr 0,81 tnr 0,75 acc 0,80 fpr 0,25 fnr 0,19 table a.4 confusion matrix for non-immigrant applications in example true condition total popula- tion condition positive condition negative total prediction predicted con- dition prediction positive prediction negative total condi- tion tpr 0,93 tnr 0,83 acc 0,90 fpr 0,17 fnr 0,07 \ufeff\x08 annex informative related open source tools b.1 general open source tools including those listed in this annex are available to audit bias and explain the output of an ai system']"
29,9,29_racism_inequality_institutional_harm,"['racism', 'inequality', 'institutional', 'harm', 'injustice', 'economics', 'industrial', 'sociological', 'stereotyped', 'terrorists']","['the term systemic bias is historically used in the context of human systems and processes operating within organizations or within society or culture and is discussed extensively in the field of industrial organization economics', 'for example systemic bias plays part in systemic racism', 'systemic racism is form of racism that can be embedded within society particular culture or an organization']"
30,8,30_privacy_experts_legislation_design,"['privacy', 'experts', 'legislation', 'design', 'rules', 'automated', 'transdisciplinary', 'regulation', 'smugglers', 'eu']","['8.3.5 unwanted bias in rule-based systems diversity in the background and experiences of the designers along with leveraging transdisciplinary experts see 8.2.4 can help in reducing chances of introducing unwanted bias into the design of system', 'source iso 3534-1:2006 1.35 3.1.2 rule-based systems knowledge-based system that draws inferences by applying set of if-then rules to set of facts following given procedures source iso/iec 2382:2015 under preparation', 'at the time of writing examples of data protection and privacy legislation include california consumer privacy act of the japanese act on the protection of personal information and the eu and eaa general data protection regulation']"
31,8,31_clause_considerations_treatment_bias,"['clause', 'considerations', 'treatment', 'bias', 'agreement', 'content', 'intellectual', 'implementation', 'requirements', 'legal']","['recognizing this different parts of this clause will apply to different implementation contexts and there can be intellectual property transparency or commercial considerations that hinder bias identification and reduction', 'the considerations and potential requirements described in this clause are not applicable to the treatment of unwanted bias alone', 'the document covers topics such as an overview of bias 5.2 and fairness 5.3 potential sources of unwanted bias and terms to specify the nature of potential bias clause assessing bias and fairness clause through metrics addressing unwanted bias through treatment strategies clause']"
32,8,32_case_criteria_recruiters_customization,"['case', 'criteria', 'recruiters', 'customization', 'compliance', 'certification', 'requirements', 'qualification', 'experience', 'capabilities']","['failure criteria can be the lower bound of acceptance in effect setting clear limits for acceptable performance for model', '\ufeff\x08 8.2.8 acceptance criteria effective requirements are testable in that when they are evaluated it is possible to determine whether system complies with them', 'establishing acceptance in terms of specific system characteristics and the degree of their compliance upfront allows for effective evaluation and decision-making']"

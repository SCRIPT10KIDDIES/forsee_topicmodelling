Topic,Count,Name,Representation,Representative_Docs
-1,240,-1_model_training_data_process,"['model', 'training', 'data', 'process', 'knowledge', 'software', 'technologies', 'confidence', 'sensor', 'validation']","['9.3.1.2 ai metrics and safety verification and validation typically metrics such as accuracy are used during training of machine learning algorithms', 'key code/models tools/libraries application graph ml model executable machine code computational hardware ml framework code/graph/ml compilers libraries figure hierarchy of technology elements ml example table continued table continued 7.2 the three-stage realization principle of an ai system summarizing iso/iec 22989:2022 figure an ai system is represented see figure by realization principle comprising three main stages data acquisition knowledge induction from data and human knowledge processing and generation of outputs', ""table ai technology elements from iso/iec 22989:2022 ai technology element ai services machine learning model development and use tools data for machine learning engineering knowledge based on domain experience tools cloud and edge computing and big data and data sources resource pool-compute storage network resource management-resource provisioning table example technology elements involved in model creation and execution for ml technology element machine learning example examples not exhaustive application graph graph exchange format gxf graph in yaml ai n't markup language yaml recently qual- ified teacher rqt graph in robot operating system rostm/ros2tm machine learning frameworka tensorflow® pytorch® keras mxnet microsoft cognitive toolkit cntk caffe theano machine learning model language open neural network exchange onnx® neural network exchange format nneftm polyml machine learning graph compiler tensorrttm glow multi-level intermediate representation mlir ngraph tensor virtual machine tvm plaidml accelerated linear algebra xla machine code compiler gcc nvcc clang/llvm sycl dpc++ opencltm openvxtm note this table does not distinguish between elements used for training and those used for inference""]"
0,79,0_61508_safety_euc_pe,"['61508', 'safety', 'euc', 'pe', 'maintenance', 'requirements', 'hazard', 'electrical', 'commissioning', 'equipment']","['iso and iec maintain terminology databases for use in standardization at the following addresses iso online browsing platform available at iec electropedia available at 3.1 safety freedom from risk 3.3 which is not tolerable source iec 61508-4:2010 3.1.11 3.2 functional safety part of the overall safety 3.1 relating to the euc equipment under control and the euc control system that depends on the correct functioning of the e/e/pe electrical/electronic/programmable electronic safety-related systems and other risk reduction measures source iec 61508-4:2010 3.1.12 3.3 risk functional safety risk functional safety combination of the probability of occurrence of harm 3.5 and the severity of that harm 3.5 note to entry for more discussion on this concept see annex of iec 61508-5', '5.2 functional safety iec 61508-4 defines functional safety as that part of the overall safety relating to the euc equipment under control and the euc control system that depends on the correct functioning of the e/e/pe electrical/ electronic/programmable electronic safety-related systems and other risk reduction measures. the e/e/ pe safety-related system is delivering safety function which is defined in iec 61508-4 as function to be implemented by an e/e/pe safety-related system or other risk reduction measures that is intended to achieve or maintain safe state for the euc in respect of specific hazardous event. in other words the safety functions control the risk associated with hazard that leads to harm to people or the environment', 'source iso 31000:2018 3.1 modified added organizational domain and note to entry 3.5 harm physical injury or damage to the health of people or damage to property or the environment source iec 61508-4:2010 3.1.1 3.6 hazard potential source of harm 3.5 source iec 61508-4:2010 3.1.2 3.7 hazardous event event that may result in harm 3.5 source iec 61508-4:2010 3.1.4 3.8 system arrangement of parts or elements that together exhibit stated behaviour or meaning that the individual constituents do not source iso/iec/ieee 15288:2023 3.46 modified removed the three notes to entry 3.9 systematic failure failure related in deterministic way to certain cause which can only be eliminated by modification of the design or of the manufacturing process operational procedures documentation or other relevant factors source iec 61508-4:2010 3.6.6 3.10 safety-related system designated system that both implements the required safety functions necessary to achieve or maintain safe state for the euc and is intended to achieve on its own or with other e/e/pe safety-related systems and other risk reduction measures the necessary safety integrity for the required safety functions source iec 61508-4:2010 3.4.1 3.11 safety function function to be implemented by an e/e/pe safety-related system or other risk reduction measures that is intended to achieve or maintain safe state for the euc in respect of specific hazardous event 3.7 source iec 61508-4:2010 3.5.1 3.12 equipment under control euc equipment machinery apparatus or plant used for manufacturing process transportation medical or other activities note to entry the euc control system is separate and distinct from the euc']"
1,68,1_risk_safety_functions_risks,"['risk', 'safety', 'functions', 'risks', 'technical', 'core', 'properties', 'focus', 'protective', 'mitigations']","['following these definitions functional safety as discipline is thus concerned with the proper engineering of these technical and non-technical safety functions for risk reduction or risk level containment of particular equipment under control from the component level up to the system level including considering human factors and under operational or environmental stress', 'based on the inclusion of other risk reduction measures in the definition of functional safety and safety functions non-technical functions are explicitly included', 'functional safety focuses on safety functions for risk reduction and the properties of these functions required for risk reduction']"
2,64,2_simulation_testing_virtual_toolchain,"['simulation', 'testing', 'virtual', 'toolchain', 'sampling', 'scenarios', 'coverage', 'simulated', 'validation', 'kpis']","['correspondence of real-world testing to simulation', 'in addition tests are evaluated with metrics to show coverage of the design domain this applies to both simulation and real-world testing', 'combination of real-world testing with simulation']"
3,55,3_safety_self_technology_functions,"['safety', 'self', 'technology', 'functions', 'mitigations', 'y2', 'cycle', 'risk', 'unsafe', 'metrological']","['functional safety life cycle for the development of an ai system is selected during functional safety planning see figure', 'key x1 input y1 y2 ... x2 output u1 u2 ... y1 input y1 y2 input y2 u1 output u1 u2 output u2 unseen tested unsafe safe figure evaluation of acceptable behaviour of ai technology meta-information from the model are used such as the internal neuron activations or by designing uncertainty measures into the network', 'in this document the view is taken that it is reasonable to start from traditional functional safety life cycle and to modify and adapt the functional safety life cycle to take into account ai system-specific issues that affect functional safety']"
4,52,4_interpretation_testing_technology_analysis,"['interpretation', 'testing', 'technology', 'analysis', 'execution', 'dynamic', 'measures', 'tables', 'ap', 'structural']","['table a.19 interpretation of modular approach reference iec 61508-3:2010 table b.9 technique or measure ref interpretation for ai system technolo- gy elements software module size limit c.2.9 those measures are applicable for use case independent elements e.g', 'interpretation for ai system technolo- gy elements functional and black box testing b.5.1 b.5.2 table a.13 applicable to ai technology elements as well', 'table a.18 interpretation of static analysis reference iec 61508-3:2010 table b.8 technique or measure ref interpretation for ai system technolo- gy elements boundary value analysis c.5.4 those measures are applicable for use case independent elements e.g']"
5,48,5_adversarial_attacks_examples_malicious,"['adversarial', 'attacks', 'examples', 'malicious', 'perturbations', 'inputs', 'generative', 'hacking', 'learning', 'gan']","['examples of this approach include high-level representation guided denoiser introduced by reference magnet which aims to detect adversarial examples and revert them to benign data using reformer network defense-gan employing generative adversarial network', 'examples of model-specific problems include adversarial machine learning and others', 'metamorphic testing data augmentation generative adversarial networks adversarial training adversarial example generation or adversarial example detection']"
6,42,6_elements_formal_diagrams_design,"['elements', 'formal', 'diagrams', 'design', 'animation', 'table', 'modelling', 'prototyping', 'refinement', 'petri']","['11b semi-formal methods table a.17 applicable to ai technology elements as well', '1b semi-formal methods table a.17 1c formal design and refinement methods b.2.2 c.2.4 computer-aided design tools b.3.5 defensive programming c.2.5 modular approach table a.19 design and coding standards c.2.6 table a.11 design standards are applicable to ai technology elements as well', 'table a.17 interpretation of semi-formal methods reference iec 61508-3:2010 table b.7 technique or measure ref interpretation for ai system technolo- gy elements logic or function block diagrams see iec 61508- 3:2010 table b.7 note applicable to ai technology elements as well']"
7,38,7_machine_parameters_models_knowledge,"['machine', 'parameters', 'models', 'knowledge', 'algorithms', 'distillation', 'ml', 'framework', 'engineers', 'application']","['alternatively parameters derived by machine learning algorithms are analysed the underlying parameters extracted and used to extend general engineering knowledge that in turn are used to develop further ai technologies', 'in other cases model parameters derived from data by machine learning algorithms are too complex to be understood analysed and verified', 'in some cases parameters derived from data by machine learning algorithms are analysed and verified after their creation']"
8,34,8_diversity_attributes_hara_bias,"['diversity', 'attributes', 'hara', 'bias', 'distribution', 'combinations', 'extracting', 'subset', 'hazard', 'risks']","['c.4 data diversity for identified risks according to reference data collection process management is used to achieve diversity of data having similar attribute values because little is known about such data diversity beforehand in usual machine- learning application', 'for for each identified set of data attributes for an identified risk checking for the existence of the test data within test data set', 'for specification of the sets of data attributes corresponding to each identified risk in the hara']"
9,29,9_images_depth_pixel_kpis,"['images', 'depth', 'pixel', 'kpis', 'weather', 'road', 'dnns', 'tivity', 'interpolate', 'misclassification']","['example the data set contains images acquired for different road types during differing weather conditions and the data acquisition takes place during daytime', 'what constitutes the training images for the neural network how are those images mapped to the operating environment', 'camera sensors around the robot provide images to neural network which produces depth image']"
10,24,10_drift_concept_drifts_data,"['drift', 'concept', 'drifts', 'data', 'training', 'changes', 'monitoring', 'seasonal', 'ood', 'runtime']","['8.4.2 issues related to environmental changes 8.4.2.1 data drift data drift is phenomenon that distribution of runtime input data departs from those used in training phase which causes degradation of performance including safety', '8.4.2.2 concept drift concept drift refers to change in relationship between input variables and model output and is accompanied by change in the distribution of the input data', 'robustness to out of distribution input is also considered for applications subject to limited training data or data drift or concept drift']"
11,22,11_verification_validation_stopping_plans,"['verification', 'validation', 'stopping', 'plans', 'frequency', 'criteria', 'process', 'procedure', 'tools', 'documented']","['the procedure is grounded on multi-step approach that includes code verification calculation verification and sensitivity analysis', 'metrics are established for checking that data set and verification and validation activities correspond to the defined domain at an early stage of development', 'amount of verification data type of verification data and how it split into relevant parameters frequency with which verification is carried out frequency with which verification data are refreshed stopping criteria for verification test plans predetermined stopping criteria process fmea the same analysis is applied to all other identified properties in table b.3 identifying set of available methods and techniques to satisfy the property']"
12,22,12_tests_statistical_criteria_combinatorial,"['tests', 'statistical', 'criteria', 'combinatorial', 'dimensional', 'structured', 'coverage', 'empirically', 'variate', 'sound']","['structured tests take place in which tests are set up for known scenarios such as on test track for automated vehicle applications', 'the general conditions for testing in these cases are similar however additional criteria for reliability of tests applies', 'in the area of software testing study several metrics for test coverage are used one possible solution is to reuse such concept for defining the coverage especially concept of combinatorial testing']"
13,21,13_pruning_training_node_activation,"['pruning', 'training', 'node', 'activation', 'neuron', 'randomized', 'weights', 'networks', 'neurons', 'scaling']","['network neuron pruning defends against training-time attacks', 'one approach to network pruning is achieved by post-training analysis of the neuron activation with clean inputs iteratively removing those that have low activation and retesting', 'however the reduced network dimensions still exceed the capacity to understand the function of each parameter in relation to its contribution to requirement satisfaction i.e']"
14,19,14_examples_text_details_type,"['examples', 'text', 'details', 'type', 'commonalities', 'rest', 'simulators', 'configuration', 'techniques', 'informative']","['the text in 9.3.3 gives some hints about what method is used', 'normative references the following documents are referred to in the text in such way that some or all of their content constitutes requirements of this document', 'annex informative possible process and useful technology for verification and validation c.1 general this annex complements the content of 9.3 by providing examples of possible process and technical methods']"
15,18,15_requirements_software_stakeholder_processes,"['requirements', 'software', 'stakeholder', 'processes', 'documenting', 'backtracking', 'debugging', 'stakeholders', 'assessments', 'environments']","['system requirements definition process', 'system requirements definition process', 'system requirements definition process']"
16,18,16_acceptance_topics_criteria_methods,"['acceptance', 'topics', 'criteria', 'methods', 'application', 'prb', 'proposing', 'treatment', 'word', 'circumstances']","['the properties are related to topics and eventually to detailed methods and techniques addressing those topics', 'acceptance criteria are identified from the set of the detailed methods and techniques', 'acceptance criteria are then identified from the set of the detailed methods and techniques']"
17,17,17_transparency_explainability_property_intellectual,"['transparency', 'explainability', 'property', 'intellectual', 'influencing', 'security', 'environmental', 'information', 'complexity', 'accountability']","['on the other hand high degree of transparency can lead to confusion due to information overload or can conflict with privacy security confidentiality requirements and intellectual properties', 'iso/iec 22989:2022 5.15.6 defines explainability as the property of an ai system to express important factors influencing the results of the ai system in way that is understandable to humans whereas transparency is defined as the property of system that appropriate information about the internal processes of an ai system is made available to relevant stakeholders-see also iso/iec tr', '8.3 degree of transparency and explainability often aspects of transparency and explainability are summarized under the term transparency']"
18,17,18_traceability_software_specification_elements,"['traceability', 'software', 'specification', 'elements', 'requirements', 'safety', 'reverification', 'specifications', 'subroutines', 'specifi']","['backward traceability between the software safety validation plan and the software safety requirements specification c.2.11 applicable to ai technology elements as well', 'forward traceability between the software safety requirements specification and software design c.2.11 applicable to ai technology elements as well', 'forward traceability between the software safety requirements specification and the software safety validation plan c.2.11 applicable to ai technology elements as well']"
19,17,19_requirements_coverage_documents_subproblem,"['requirements', 'coverage', 'documents', 'subproblem', 'standards', 'deterioration', 'examination', 'requirement', 'attributes', 'engineering']","['for middle-level requirements in addition to those described for low-level requirements analysis of risks of deterioration of quality in use in overall system and their impact with certain level of engineering coverage and record the results in documents', 'for middle-level requirements in addition to those described for low-level requirements ensuring data corresponding to particularly relevant risk factors to satisfy in principle the international standards for pair-wise coverage', 'for high-level requirements in addition to those described for middle-level and low-level requirements acquiring certain indicators for coverage of data included in each case']"
20,15,20_complexity_applications_trees_machine,"['complexity', 'applications', 'trees', 'machine', 'models', 'learning', 'probabilistic', 'data', 'adaptability', 'assistants']","['for more and more applications adopting machine learning as an ai technology is enabling the rapid and successful development of functions that detect trends and patterns in data', 'in situations where an interpretable result is desired tools such as optimal classification trees or born-again tree ensembles are applied to reduce complexity and allow for human expert review', 'successful ml applications are found in analysis of for example financial data social networking applications and language recognition image recognition particularly face recognition healthcare management and prognostics digital assistants manufacturing robotics machine health monitoring and automated vehicles']"
21,15,21_references_al_goodfellow_rearrangement,"['references', 'al', 'goodfellow', 'rearrangement', 'gama', 'cited', 'amendments', 'comprehensive', 'issues', 'additional']","['see references', 'see references and', 'for undated references the latest edition of the referenced document including any amendments applies']"
22,14,22_clause_factors_degree_maturity,"['clause', 'factors', 'degree', 'maturity', 'technology', 'hardware', 'specifications', 'complexity', 'transparency', 'qualitative']","['details of the properties and risk factors of systems using ai technology and their related topics and challenges are discussed in this clause', 'such properties and risk factors include degree of automation and control see 8.2 degree of decision transparency and explainability see 8.3 environmental complexity and vagueness of their defining specifications see 8.4 resilience to adversarial inputs see 8.5 system hardware considerations see 8.6 and technological maturity see 8.7', 'these factors are described further in clause and include the level of automation and control see 8.2 the degree of decision transparency and explainability see 8.3 the complexity of the environment and vague specifications see 8.4 security see 8.5 system hardware issues see 8.6 and the maturity of the technology see 8.7']"
23,14,23_attention_maps_prediction_explanations,"['attention', 'maps', 'prediction', 'explanations', 'trainable', 'saliency', 'convolution', 'captioning', 'feature', 'layers']","['post-hoc attention maps for sanity checking and feature manipulation attention map also known as saliency map an explanation method used for interpreting the predictions of cnns or sensitivity map is common type of machine learning explanation to point out the most important feature in given prediction', 'there are several considerations on attention mechanisms attention mechanism to learn global context the attention mechanism learns the relationship between sequence of features e.g', 'trainable attention trainable attention mechanisms have attention weights that are learned during training to improve attention efficiency']"
24,14,24_unsafe_controller_control_detection,"['unsafe', 'controller', 'control', 'detection', 'switch', 'barrier', 'states', 'prohibit', 'inertia', 'hybrid']","['the back-up action allows the use of detection methods to switch the output when unsafe conditions are detected', 'constraining the output based on function of input is not usually appropriate for systems with dynamics where the system state defines the unsafe regions but does not instantly respond to change in the controller input', 'these sets are typically conservative as they do not actively look to recover system back towards safer regions thus control barrier functions are used as detection mechanisms to switch in conventional stabilizing controls producing control signal if they are available with suitable ai monitoring as described in 10.2.1']"
25,13,25_pe_automated_function_grey,"['pe', 'automated', 'function', 'grey', 'diagnostic', 'automotive', 'safety', 'layer', 'offline', 'impact']","['the ai technology used in this system is considered of usage level a1 as described in 6.2 because it is used in safety relevant e/e/pe system and automated decision-making of the ai system is possible', 'an example of classification of usage level is as follows usage level a1 is assigned when the ai technology is used in safety-relevant e/e/pe system and where automated decision-making of the system function using ai technology is possible usage level a2 is assigned when the ai technology is used in safety-relevant e/e/pe system and where no automated decision-making of the system function using ai technology is possible e.g', 'the dark grey components are considered usage level a1 as described in 6.2 since they are used in safety relevant e/e/pe system and automated decision-making of the ai is possible']"
26,13,26_neural_errors_vulnerability_classifiers,"['neural', 'errors', 'vulnerability', 'classifiers', 'soft', 'inputs', 'perturbations', 'faults', 'random', 'imperceptible']","['the ensemble use of neural networks to build reliable classifiers see reference the idea is to combine several weak classifiers to obtain strong one so that the classifier still works reliably if one of its members fails', 'for hardware errors fault-aware training that includes error modelling during neural network training makes neural networks more resilient to specific fault models on the device see reference', '9.4.4 \u200bevaluation of vulnerability to hardware random failures it has been shown that the vulnerability of deep neural networks to soft errors is low see references']"
27,13,27_abstraction_propagation_faults_models,"['abstraction', 'propagation', 'faults', 'models', 'inaccessible', 'chains', 'manifold', 'systematic', 'lead', 'manually']","['by applying the fault model to all elements the completeness with respect to the fault model abstraction level is ensured', 'the idea of fault models fault awareness is to cover the manifold details of reality by sufficiently high abstraction level', '11.5.2 fault models the concept of fault models is intended to enable systematic and possibly automated analysis of an element behaviour in the presence of faults']"
28,13,28_automation_intervention_control_autonomy,"['automation', 'intervention', 'control', 'autonomy', 'equipment', 'agent', 'capable', 'heteronomy', 'onboard', 'truck']","['high automation the system performs parts of its mission without external intervention', '8.2 level of automation and control the level of automation sometimes called levels of autonomy in the literature describes the extent to which an ai system functions independently of human supervision and control', 'table relationship among autonomy heteronomy and automation derived from iso/iec 22989:2022 table level of automation comments automated system autonomous autonomy the system is capable of modifying its operating domain or its goals without external intervention control or oversight']"
29,13,29_monitor_diversity_redundancy_monitored,"['monitor', 'diversity', 'redundancy', 'monitored', 'recovery', 'storage', 'software', 'fault', 'algorithm', 'multiple']","['this is related to multiple ai technologies exhibiting the same behaviour but implemented by different teams using separate labelling rules using different problem formulations using different training data executing on diverse hardware also valid for non-ai technology specific failure modes with diversity of sensing with diversity of self-check or self-validation methods with diversity of ai technology itself', '4a re-try fault recovery mechanisms c.3.7 it is also used for ai technology in principle subject to sufficient storage state space and increases the robustness of an ai technology result as well since such methodology introduces kind of redundancy slight changes in the input vector', '3c diverse monitor techniques with separa- tion between the monitor computer and the monitored computer c.3.4 3d diverse redundancy implementing the same software safety requirements spec- ification c.3.5 3e functionally diverse redundancy imple- menting different software safety require- ments specification c.3.5 3f backward recovery c.3.6 it is also used for ai technology in principle subject to sufficient storage state space and increases the robustness of an ai result as well since such method- ology introduces kind of redundancy slight changes in the input vector']"
30,13,30_realization_stages_properties_knowledge,"['realization', 'stages', 'properties', 'knowledge', 'processing', 'verifiability', 'table', 'criteria', 'interpretability', 'acceptance']","['the process in which properties are selected is described in figure for each of the three stages of the three-stage realization principle', 'figure three-stage realization principle 7.3 deriving acceptance criteria for the three-stage of the realization principle typically see for example in iso/iec 22989:2022 three-stage realization principle see figure is used to derive acceptance criteria desirable properties are defined for each of the three stages', 'table b.3 mapping of properties to the realization principle stages acquisition from inputs or data knowledge induction from training data and human knowledge processing and genera- tion of outputs specifiability domain shift verifiability robustness interpretability explainability table b.4 provides an example analysis from the third stage of the three-stage realization principle in relation to the verifiability property']"
31,12,31_products_29119_constitute_commercially,"['products', '29119', 'constitute', 'commercially', 'trademark', 'endorsement', 'ieee', 'validated', 'functionality', 'limitation']","['the trade name or trademark in this table are examples of suitable products available commercially', 'this information is given for the convenience of users of this document and does not constitute an endorsement by iso or iec of these products', 'this information is given for the convenience of users of this document and does not constitute an endorsement by iso or iec of these products']"
32,12,32_table_measure_61508_electronics,"['table', 'measure', '61508', 'electronics', 'references', 'coding', 'descriptions', 'acquisition', 'modification', 'module']","['table a.2 interpretation of software design and development software architecture design reference iec 61508-3:2010 table a.2 technique or measure ref', 'table a.9 interpretation of software verification reference iec 61508-3:2010 table a.9 technique or measure ref', 'table a.4 interpretation of software design and development detailed design reference iec 61508-3:2010 table a.4 technique or measure ref']"
33,12,33_supervisor_supervisory_supervision_control,"['supervisor', 'supervisory', 'supervision', 'control', 'constraints', 'outputs', 'unsafe', 'restrictive', 'explicit', 'curation']","['key intelligent control unsafe safety verification input upper/lower bounds output limited output unsafe limited constrained safe figure safer application of ai technology to control through supervisory constraints on discrete outputs', 'safe subset of the action space is determined priori or online using supervisor function with constraints or limits', '10.2.3 use of supervision function with constraints to control the behaviour of system to within safe limits with an appropriate supervisory module it is possible that an ai system is constrained to work within predefined safe envelope']"
34,12,34_standards_class_functional_safety,"['standards', 'class', 'functional', 'safety', 'technology', 'application', 'level', 'using', 'achievement', 'methods']","['for ai technology class components application of existing functional safety international standards is possible and desired in general', 'table example of ai classification table ai technology class ai application and usage level ai technology class ai technology class ii ai technology class iii usage level a1 application of risk reduction concepts of existing func- tional safety international standards possible appropriate set of re- quirements at the time of publication of this document no ap- propriate set of properties with related methods and techniques is known to achieve sufficiently reduc- tion of risk usage level a2 appropriate set of re- quirements usage level b1 appropriate set of re- quirements usage level b2 appropriate set of re- quirements usage level appropriate set of re- quirements usage level application of risk reduction concepts of existing functional safety international standards static offline during development teaching or learning only', 'this axis considers the level of fulfilment of ai technology in satisfying the identified set of properties in which class is assigned if ai technology is developed and reviewed using existing functional safety international standards for example if the properties and the set of methods and techniques leading to achievement of the properties are identified using existing functional safety international standards class ii is assigned if ai technology can not be fully developed and reviewed using existing functional safety international standards but it is still possible as shown in figure to identify additional complementary requirements methods and techniques for development design verification and validation of the desired safety properties to achieve the necessary risk reduction class iii is assigned if ai technology can not be developed and reviewed using existing functional safety international standards and it is also not possible to satisfy the set of identified properties with related methods and techniques']"
35,11,35_models_adequateness_flexible_simplify,"['models', 'adequateness', 'flexible', 'simplify', 'logic', 'relevance', 'extent', 'complexity', 'parameters', 'application']","['the extent to which the adequateness of models for the intended application is considered', 'the same holds for the logic involved in the translation of the models and the parameters', 'the complexity and level of detail for each model varies depending on the relevance significance and range of each factor']"
36,11,36_code_cuda_libraries_expressiveness,"['code', 'cuda', 'libraries', 'expressiveness', 'describ', 'arrays', 'fpgas', 'representation', 'programmable', 'software']","['cuda c++ libraries as well as for the code describ- ing the model even if the expressiveness is not the same as in traditional code', 'cuda c++ libraries as well as for the code describing the model even if the expressiveness is not the same as in traditional code', 'cuda c++ libraries as also for the code describing the model even if the expressiveness is not the same as in traditional code']"
37,11,37_differences_challenges_aspects_validating,"['differences', 'challenges', 'aspects', 'validating', 'residual', 'refer', 'clauses', 'methodologies', 'measures', 'relevance']","['measures introduced in this clause are directed by knowledge of these failure modes and are introduced as part of robust ml process described in clause', 'this clause addresses four significant aspects of such differences although potential differences are not limited to those described in this clause see reference for additional examples', 'the failure mechanisms from clause have highlighted differentiating challenges for ml components while clause addresses the through-life process of verifying and validating these components']"
38,10,38_persons_detects_behaviour_rewards,"['persons', 'detects', 'behaviour', 'rewards', 'act', 'mape', 'decides', 'sweep', 'intentions', 'scientific']","['on the other hand many ai technologies are considered as black box as their internal behaviour and the basis of their decision-making processes are difficult for human to understand', 'for example an ai system that detects persons based on camera sweep decides that it achieves very high rewards if it constantly detects persons and thus follows them around with its sensors potentially missing critical events in other affected areas', 'such techniques sometimes called grey box approaches are useful for the understanding of the behaviour of an ai system especially when it differs in its decision-making from the implementors intentions']"
39,10,39_devices_programmable_controllers_flowchart,"['devices', 'programmable', 'controllers', 'flowchart', 'classification', 'computer', 'electrical', 'microprocessors', 'plcs', 'axes']","['figure flowchart for determining classification of ai technology ai technology elements and the three-stage realization principle 7.1 technology elements for ai model creation and execution the creation and the execution of an ai model involves different ai technology elements', 'example electrical/electronic/programmable electronic devices include electro-mechanical devices electrical solid-state non-programmable electronic devices electronic electronic devices based on computer technology programmable electronic', 'example the following are all programmable electronic devices microprocessors micro-controllers programmable controllers application specific integrated circuits asics programmable logic controllers plcs other computer-based devices e.g']"
40,10,40_data_events_mitigation_input,"['data', 'events', 'mitigation', 'input', 'training', 'risk', 'rebalance', 'alterations', 'unfeasible', 'incidence']","['common examples include picking inappropriate training data data whose distribution does not reflect the actual distribution encountered in the application context or omitting important examples in the training data', 'this means that if the training data set contains some data that are intended to work as mitigation for particular risk its influence on the trained model is not certain nor tested separately for each risk', 'if some events are low probability but have high impact it is appropriate to increase the incidence of these events in the input data making the frequency of events in the input proportional to the risk rather than the actual probability of occurrence']"
41,10,41_smoothing_robustness_randomized_models,"['smoothing', 'robustness', 'randomized', 'models', 'noise', 'regularisation', 'numerically', 'regularization', 'randomization', 'guarantees']","['regularisation or randomized training numerically and directly evaluating the robustness e.g', 'ensuring that trained models have sufficient robustness in terms of the given problem using the following approaches generating multiple models of different sizes using smaller models so long as other objectives are met large models lead to excess sensitivity applying technology that improves robustness e.g', 'randomization approaches such as randomized smoothing provides an efficient equivalent to training multiple modes with data augmented with randomized noise so as to calculate the final result value to be mean with respect to the noise distribution']"
42,9,42_labels_annotated_annotators_annotator,"['labels', 'annotated', 'annotators', 'annotator', 'annotating', 'policy', 'obfuscation', 'supervised', 'annotation', 'specification']","['the amount of of randomly selected data are additionally annotated by third annotator', 'encryption obfuscation of labels and data distribution', 'each image is annotated by two independent annotators']"
43,8,43_explanations_explainability_recipient_completeness,"['explanations', 'explainability', 'recipient', 'completeness', 'interpretability', 'information', 'unpredictable', 'heuristic', 'correctness', 'analysing']","['considerations include whether sufficient information about the system is available whether it is understandable or at least delivers comprehensible information possibly indirectly to the intended recipient the intended recipient of an explanation varies depending on the context', 'explanation truthfulness since model explanations are always incomplete explanations of black box models the correctness and completeness of explanations is greatly influenced by factors like the heuristic technique input example and training data size and quality', 'high-level of explainability protects against unpredictable behaviour of the system but is sometimes accompanied by lower overall performance in terms of the quality of decisions due to the limitation of current explainability technology which limits the amount of information contained in the model to create explanations of reasonable size']"
44,8,44_failures_runtime_faults_redundancy,"['failures', 'runtime', 'faults', 'redundancy', 'benchmarks', 'systematic', 'insights', 'hazards', 'technologies', 'extension']","['for higher effectiveness redundancy is combined with diversity to reduce the likelihood of systematic failures during development', 'for this reason the catalogue of available measures for dealing with systematic failures is extended with respect to the specificities of ai technologies annex provides an example of that extension', 'four points are considered for the development of monitors the type of ai technology faults that is detected the ways in which ai technology faults are revealed at runtime the performance benchmarks of different runtime monitors the types of intervention that are used to circumvent fault after detection and also circumvent the potential hazards invoked']"
45,8,45_class_classified_abstractions_belongs,"['class', 'classified', 'abstractions', 'belongs', 'justification', 'described', 'classification', 'defined', 'technologies', 'information']","['the use of this statistical information is considered in the justification for class ii and class iii ai technologies', 'deep learning models are more likely classified as class ii or class iii', 'ai technology class']"
46,8,46_training_phase_validation_50128,"['training', 'phase', 'validation', '50128', 'intentions', 'overfitting', 'fitting', 'mitigating', 'cross', 'testing']","['intentions of testing activities in the verification and validation phase means of mitigating training errors is also considered during the training phase', 'checking that there are sufficient training data and test data for each analysed case in the training phase validation phase etc', 'one known way of achieving this is to ensure the independence between training data and test data which is enforced through development process management and assessment tool-based approaches or even using level of independence in the teams or organizations carrying out the testing see iec 61508-1 clause or bs en 50128:2011 clause']"

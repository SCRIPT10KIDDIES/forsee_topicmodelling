Topic,Count,Name,Representation,Representative_Docs
-1,110,-1_anne_driven_approaches_knowledge,"['anne', 'driven', 'approaches', 'knowledge', 'learning', 'computational', 'logistic', 'planning', 'optimization', 'engineering']","['common optimization algorithms include gradient descent method early stopping batch gradient descent stochastic gradient descent mini- batch gradient descent gradient descent methods utilizing the coefficient of momentum root mean square propagation and adaptive moment optimization', 'figure ai computational approaches 7.2 knowledge-driven approaches knowledge-driven approaches mimic the functions of human intelligence from symbols and logic rules', 'typical data-driven computational approaches include machine learning which can be decomposed as linear or logistic regression probabilistic graphical model decision tree neural networks and other approaches']"
0,43,0_layer_input_cnns_convolution,"['layer', 'input', 'cnns', 'convolution', 'networks', 'cnn', 'neuron', 'ffnn', 'features', 'signals']","['cnns include an input layer and combination of convolution layers activation layers pooling layer fully connected layer and normalization layers', 'key input layer hidden layer output layer figure example of an ffnn structure 8.4.8.2 main characteristics ffnns have strong fitting ability and can approximate common continuous nonlinear functions which can be used for complex feature transformation or approximation of complex conditional distribution', 'the layers between input layer and output layer are hidden layers']"
1,33,1_genetic_genomes_selection_crossover,"['genetic', 'genomes', 'selection', 'crossover', 'mutation', 'metaheuristics', 'breeding', 'algorithm', 'swarm', 'evolutionary']","['the child genome comprises the first part of one parent genome before the cut point and the second part of the other parent genome after the cut point', 'here genomes in the population are scored with the fitness function and randomly selected for breeding with the probability of selection proportional to the fitness', '8.5.2.2 main characteristics solving an optimization problem using genetic algorithm involves being able to represent potential solutions to the problem on genome and being able to evaluate genome using fitness function which scores the genome on how well it solves the problem']"
2,31,2_computational_clause_dynamic_overview,"['computational', 'clause', 'dynamic', 'overview', 'discriminative', 'trained', 'list', 'agents', 'architecture', 'criteria']","['in this document state of the art of selected algorithms and approaches used in ai system is described in terms of theories and techniques main characteristics and typical applications', '\ufeff\x08 vi information technology artificial intelligence ai overview of computational approaches for ai systems scope this document provides an overview of the state of the art of computational approaches for ai systems by describing main computational characteristics of ai systems main algorithms and approaches used in ai systems referencing use cases contained in iso/iec tr', 'to reflect state-of-the-art methods used in ai this document is structured as follows clause provides an overall description of computational approaches in ai systems clause discusses the main characteristics of ai systems clause provides general taxonomy of computational approaches including knowledge-driven and data-driven approaches clause discusses selected algorithms used in ai systems including basic theories and techniques main characteristics and typical applications']"
3,29,3_tree_ensemble_forest_induction,"['tree', 'ensemble', 'forest', 'induction', 'bootstrap', 'importance', 'forests', 'attributes', 'supervised', 'bag']","['the out of bag sample oi for the ith decision tree is the set of data points from the training set that are not selected in bootstrap sample bi', 'the ith decision tree ti in the ensemble is constructed from bootstrap sample bi of the training set', 'figure example of decision tree 8.4.2.2 main characteristics decision tree induction algorithms build the decision tree for specific training data set']"
4,28,4_regression_coefficients_logistic_dependent,"['regression', 'coefficients', 'logistic', 'dependent', 'hyperplane', 'values', 'assumed', 'errors', 'b2', 'assess']","['8.4.5.3 typical applications logistic regression is used for small data sets where there is linear relationship between independent and dependent variables', '8.4.4.3 typical applications linear regression is used when there is linear relationship between independent and dependent variables and one needs to find the value of the dependent variable', '8.4.4 linear regression 8.4.4.1 theories and techniques linear regression is regression method that builds function of independent variables to predict some target variables values']"
5,27,5_learning_unsupervised_labelled_supervised,"['learning', 'unsupervised', 'labelled', 'supervised', 'unlabelled', 'erm', 'estimation', 'srm', 'dimensionality', 'sets']","['learning from examples involves supervised learning approaches that learn machine learning model from labelled data', 'semi-supervised learning approaches use both labelled data and unlabelled data', 'other learning approaches include unsupervised learning which involves identifying the natural structure of data sets semi-supervised learning which deals with partially labelled data sets online learning algorithms which continue to learn as they receive data networks and relational learning ranking and preference learning representation learning transfer learning and active learning']"
6,23,6_knowledge_fusion_extraction_kgs,"['knowledge', 'fusion', 'extraction', 'kgs', 'entities', 'concepts', 'ontology', 'structured', 'modelling', 'semantic']","['knowledge storage the objects of knowledge storage include basic attribute knowledge relation knowledge event knowledge temporal knowledge and resource knowledge', 'knowledge fusion knowledge fusion usually extracts knowledge from data level and concept level', 'the underlying computational process of kg generally includes knowledge extraction knowledge representation knowledge storage knowledge modelling knowledge fusion and knowledge computation']"
7,22,7_storage_sensors_data_static,"['storage', 'sensors', 'data', 'static', 'applications', 'humidity', 'microprocessors', 'microphones', 'phone', 'streaming']","['smart bracelet smart watch smart phone iot sensors e.g', 'considerations for data used in ai systems include acquisition storage and access', 'further specialized sensor requirements are needed for smart home smart medical and smart security applications']"
8,21,8_web_semantic_rdf_syntaxes,"['web', 'semantic', 'rdf', 'syntaxes', 'relationships', 'owl', 'entities', 'entity2', 'nodes', 'xml']","['examples of syntaxes for the semantic web include xml-based syntaxes whose usability varies .ttl or turtle is generally preferred by practitioners formal logic-based syntaxes that resemble logical formulae syntaxes that are partly in natural language manchester syntax for improved human readability syntaxes that are fully readable as natural language fully defined but not yet implemented by tools such as sydney syntax which help domain experts perform knowledge checking without having to understand technical syntax', 'the most prevalent kind of ontology as knowledge structure is that implemented by the w3c semantic web technology stack based on rdf and owl', 'it is knowledge graph which utilizes the semantic web technology stack based on rdf']"
9,20,9_lstm_rnn_long_rnns,"['lstm', 'rnn', 'long', 'rnns', 'gradient', 'memory', 'spatial', 'orders', 'sequences', 'modelling']","['the lstm networks mitigate the problem of gradient explosion or gradient disappearance in simple rnns', 'relatively long input sequence will cause the problem of gradient explosion and disappearance also known as long-term dependence problem', '8.4.10 long short-term memory network 8.4.10.1 theories and techniques lstm network is special rnn that can learn long-term dependence information']"
10,20,10_bert_xlnet_bidirectional_autoregressive,"['bert', 'xlnet', 'bidirectional', 'autoregressive', 'token', 'textual', 'gpt', 'permutation', 'encoder', 'qnli']","['8.4.14 bidirectional encoder representations from transformers 8.4.14.1 theories and techniques bert is kind of language model that uses unlabelled training data to obtain rich semantic representation of the text', '8.4.14.3 typical applications bert has contributed to the advancement of nlp tasks such as corpus of linguistic acceptability cola microsoft research paraphrase corpus mrpc multi-genre natural language inference multinli question natural language inference qnli quora question pairs qqp recognition textual entailment rte semantic textual similarity benchmark sts-b and stanford sentiment treebank sst-2', '8.4.15 xlnet 8.4.15.1 theories and techniques further improving on gpt and bert xlnet is an autoregressive permutation language model']"
11,19,11_loss_functions_classification_regression,"['loss', 'functions', 'classification', 'regression', 'data', 'criteria', 'predict', 'models', 'algorithms', 'sales']","['machine learning method includes three basic elements loss functions learning criteria and optimization algorithm', 'from loss function perspective machine learning methods can be classified as linear or nonlinear', 'common loss functions include 0-1 loss function quadratic loss function cross-entropy loss function hinge loss function mean absolute error loss function huber loss function log-cosh loss function and quantile loss function']"
12,18,12_reasoning_deductive_mathematics_premises,"['reasoning', 'deductive', 'mathematics', 'premises', 'inductive', 'methods', 'generalizing', 'forensics', 'aristotle', 'arguments']","['pure mathematics concerns only deductive inference if mathematical proof is valid then the conclusions always logically follow from the premises4', 'this enables new deductive forms of reasoning', '8.3.3 deductive inference deductive reasoning is form of reasoning which starts with set of propositions or assertions known as premises or axioms and uses only reasoning methods which guarantee that if the premises are true any conclusions made are also true']"
13,17,13_computing_processors_cloud_fpga,"['computing', 'processors', 'cloud', 'fpga', 'iot', 'frameworks', 'processor', 'gpus', 'silicon', 'asic']","['6.3.2 infrastructure-based ai systems can face simultaneous challenges in computing platform design optimization computing efficiency in complex heterogeneous environments highly parallel and scaled computing frameworks and the computing performance of ai applications', 'with the growth of iot systems capable of general-purpose computing on gpus and multi-core cpus or ai processing on application-specific processors and accelerators ai system adaptability now extends to iot implementation considerations such as near-real-time data processing optimization for low latency and power-efficient performance', 'abbreviated terms ai artificial intelligence asic application-specific integrated circuit bert bidirectional encoder representations from transformers bptt back propagation through time cnn convolutional neural network cpu central processing unit dag directed acyclic graph dnn deep neural network erm empirical risk minimization ffnn feedforward neural network fpga field programmable gate array gdm gradient descent method gpu graphics processing unit \ufeff\x08 gpt generative pre-training iot internet of things kg knowledge graph knn k-nearest neighbour lstm long short-term memory mfcc mel-frequency cepstrum coefficient mlm masked language model ner named entity recognition nlp natural language processing nsp next sentence prediction owl web ontology language qa question answering rdf resource description framework rnn recurrent neural network rtrl real-time recurrent learning sparql sparql protocol and rdf query language sql structured query language srm structure risk minimization svm support vector machine uri uniform resource identifier xml extensible markup language general advances in computational approaches are an important driving force in the maturation of ai to become capable of processing various tasks']"
14,16,14_reinforcement_learning_approaches_control,"['reinforcement', 'learning', 'approaches', 'control', 'actions', 'robot', 'automated', 'gaming', 'chess', 'mappings']","['for instance deep learning approaches can be either supervised or unsupervised reinforcement learning can be achieved through deep learning and approaches for machine translation or object recognition can be learning approaches', 'approaches can be grouped into learning from examples knowledge- based learning probabilistic learning reinforcement learning deep learning approaches gans and other learning approaches', 'reinforcement learning']"
15,16,15_tr_cases_machine_use,"['tr', 'cases', 'machine', 'use', 'approaches', 'terminology', 'computational', 'algorithms', 'behaviour', 'framework']","['deep learning machine learning and neural networks are among the most frequently mentioned computational approaches in ai use cases collected in iso/iec tr', 'machine learning algorithms and approaches are mentioned in most iso/iec tr use cases', 'iso and iec maintain terminology databases for use in standardization at the following addresses iso online browsing platform available at iec electropedia available at 3.1 heuristic search search based on experience and judgment used to obtain acceptable results without guarantee of success source iso/iec 2382:2015 modified notes to entry removed']"
16,16,16_gans_adversarial_samples_generative,"['gans', 'adversarial', 'samples', 'generative', 'real', 'image', 'generators', 'gan', 'discriminators', 'perceptron']","['convergence occurs when the discriminator network no longer accurately classifies samples as real or fake such that the generator network is producing samples that conform to the real data distribution', 'technical report 3.4 discriminator neural network that classifies samples usually produced by generator note to entry discriminators primarily appear in the context of generative adversarial networks', '3.3 generator neural network that produces samples usually to be classified by discriminator note to entry generators primarily appear in the context of generative adversarial networks']"
17,16,17_hypothesis_conclusion_symptoms_false,"['hypothesis', 'conclusion', 'symptoms', 'false', 'condition', 'consistent', 'contradiction', 'affirming', 'reasoning', 'rests']","['however it diagnostic reasoning doctors often take cluster of symptoms to indicate the presence of specific condition and make diagnosis saying that symptoms and are consistent with condition', 'it shows that the hypothesis is not true by showing that the hypothesis and conclusion are not logically consistent with each other', 'in mathematics the pattern of taking hypothesis assuming it is true and reasoning to false conclusion is called proof by contradiction']"
18,15,18_theory_empirically_proven_science,"['theory', 'empirically', 'proven', 'science', 'inductive', 'mathematics', 'scientific', 'observations', 'abstract', 'hypothesis']","['it took some time but eventually it was conclusively proven through empirical methods and has been accepted as fact for some hundreds of years to everyone who understands basic science', 'in some fields such as number theory some hypotheses have testable predictions about how numbers behave but in most fields of pure mathematics there is nothing that can be empirically tested as cross-check to the proof', 'there are cases in which an inductive theory becomes fact but only when it concerns something that can be empirically proven they are bounded in time and space rather than being abstract generalisations']"
19,15,19_transfer_domains_learning_transductive,"['transfer', 'domains', 'learning', 'transductive', 'transferring', 'translate', 'adaptable', 'text', 'target', 'applications']","['8.4.13.2 main characteristics transfer learning is based on identifying opportunities to apply existing knowledge to new domain transferring labelled data or knowledge structures e.g', '\ufeff\x08 8.4.13 transfer learning 8.4.13.1 theories and techniques transfer learning methods store and abstract knowledge gained from training data for given problem and apply this knowledge to different problem', 'transfer learning']"
20,15,20_logic_knowledge_uncertain_predicate,"['logic', 'knowledge', 'uncertain', 'predicate', 'fuzzy', 'symbols', 'logics', 'planning', 'commit', 'conceptualized']","['3.2 fuzzy logic fuzzy-set logic nonclassical logic in which facts inference rules and quantifiers are given certainty factors source iso/iec 2382:2015 modified notes to entry removed', 'knowledge representation knowledge representation is kind of data structure for describing knowledge using predicate logic if-then generation and framework representation', 'knowledge representation is kind of data structure for describing knowledge using predicate logic if-then generation and knowledge frame representation']"
21,15,21_inference_land_bayesian_likelihood,"['inference', 'land', 'bayesian', 'likelihood', 'causal', 'tosses', 'statistics', 'variational', 'inferences', 'peaking']","['bayesian inference is the act of taking statistical inference from bayesian point of view', 'the subject matter of statistics is probability thus statistical inference primarily concerns the notion of likelihood and the probability that certain future event will or won happen', '8.3.5 bayesian inference bayesian inference is form of statistical inference']"
22,15,22_speech_recognition_vectors_input,"['speech', 'recognition', 'vectors', 'input', 'phonemes', 'acoustic', 'sequences', 'example', 'language', 'machine']","['the synchronous sequence-to-sequence task is mainly used for sequence labelling where input and output are applied to each sample the length of input and output sequences are the same', 'figure multi-step learning-based speech recognition for end-to-end learning-based speech recognition as shown in figure the entire process from feature extraction to phoneme expression can be directly completed by dnn', 'taking speech recognition as an example in multi-step speech recognition as shown in figure speech is converted into speech feature vectors e.g']"
23,14,23_logic_propositional_quantifiers_transitive,"['logic', 'propositional', 'quantifiers', 'transitive', 'assertions', 'propositions', 'statements', 'modifiers', 'notions', 'doxastic']","['first order logic', 'whereas in propositional logic one only makes assertions such as socrates is person in first order logic one can say there exists an such that is socrates and is person', 'the key difference between first order logic and propositional logic is that first order logic uses variables and quantifiers']"
24,12,24_knn_neighbours_nearest_label,"['knn', 'neighbours', 'nearest', 'label', 'metrics', 'attributes', 'classification', 'normalized', 'svms', 'mahalanobis']","['8.4.6.2 main characteristics as knn depends on distance measures it is generally recommended that the values of continuous attributes are normalized to keep certain attributes from overwhelming the resulting distance measure', 'the nearest neighbours are found through distance metrics', 'knn makes prediction for new data point by finding the nearest neighbours in the training data']"
25,12,25_search_stochastic_algorithms_heuristic,"['search', 'stochastic', 'algorithms', 'heuristic', 'assumptions', 'adversarial', 'observable', 'opponent', 'situations', 'nondeterministic']","['these approaches can be further divided into various types of search classical advanced search algorithms adversarial search and constraint satisfaction', 'advanced search algorithms include those that search in local subspace those that are nondeterministic those that search with partial observation of the search space and online versions of search algorithms', 'classical search algorithms solve problems by search over some state space and can be divided into uninformed searches and heuristic searches which apply rule of thumb to guide and speed up the search']"
26,11,26_event_na誰ve_bayes_petal,"['event', 'na誰ve', 'bayes', 'petal', 'probabilities', 'bayesian', 'attribute', 'classification', '75', 'threshold']","['for example if petal width 0,8 then class setosa or if petal width 0,8 and petal length 4,75 then class versicolor', 'in na誰ve bayesian classification it is assumed that the attributes are independent from each other as shown in formula the posterior probability of c|a is then calculated based on the likelihood ai|c and the prior', 'the na誰ve bayes model uses the bayesian theory for learning and inference as shown in formula where b|a is the probability of event given event a|b the probability of event given event and are the probabilities of events or b. where b|a is the probability of event given event a|b is the probability of event given event and are the probabilities of events or']"
27,10,27_attributes_test_gini_index,"['attributes', 'test', 'gini', 'index', 'importance', 'statistical', 'calculates', 'permuting', 'rank', 'chooses']","['by averaging over multiple permutations and trees the algorithm calculates mean decrease in error which is then used to rank the importance of attributes', 'these computational approaches involve reducing the number of dimensions of data by either dimensionality reduction feature extraction algorithms which identify new smaller number of attributes to represent data or feature selection which chooses subset of the most appropriate attributes', 'similarly by permuting the values of particular attribute and calculating the difference in test error for tree before and after the permutation it is possible to estimate the importance of the attribute']"
28,9,28_nlp_speech_tagging_tasks,"['nlp', 'speech', 'tagging', 'tasks', 'text', 'retrieval', 'robotics', 'segmentation', 'mlp', 'syntactic']","['supervised learning is used in nlp information retrieval text mining handwriting recognition spam detection and other fields', 'computation approaches in these areas are associated with the fields of nlp including tasks such as language modelling text classification information retrieval information extraction parsing machine translation and speech recognition computer vision including image processing and object recognition and robotics', 'by performing joint training with mlp and nlp tasks the machine learning model will output the vector representation of each word which describes comprehensively and accurately the overall information of the input text whether single sentence or sentence pair']"
29,9,29_wet_raining_observation_conclusion,"['wet', 'raining', 'observation', 'conclusion', 'elephant', 'bathing', 'implausibly', 'hosed', 'deductively', 'day']","['the path being wet is logically consistent with it having rained this morning but it doesn prove it', 'premise if it has been raining this morning the path will be wet', 'empirical observation is the path wet']"
30,8,30_ontology_cognitive_people_concepts,"['ontology', 'cognitive', 'people', 'concepts', 'knowledge', 'compositional', 'axiom', 'philosophy', 'ontologies', 'foundational']","['however an ontology as knowledge structure is essentially knowledge model of whatever domain people wish to declare knowledge about', 'an ontology model consists of specifically defined concepts that relate to compositional elements such as structure entity term attribute function and axiom', '8.2.2 ontology 8.2.2.1 theories and techniques ontology is term which originates from philosophy where it refers to what people assert to exist']"
31,8,31_nodes_tests_children_binary,"['nodes', 'tests', 'children', 'binary', 'partitioning', 'leaf', 'mixture', 'attributes', 'data', 'categorical']","['nodes associated with numeric attributes are often split into two children with binary test as shown in figure', 'nodes associated with categorical attributes either use binary test i.e', 'the partitioning is described as dag where nodes within the graph are associated with attribute tests and leaf nodes correspond to either class value or regression value']"

Topic,Count,Name,Representation,Representative_Docs
-1,107,-1_dataset_decommissioning_labelling_inspection,"['dataset', 'decommissioning', 'labelling', 'inspection', 'analytics', 'samples', 'requirements', 'planning', 'provisioning', 'process']","['sub-sampling khz audio stream to khz select random students from in course vs. selecting each from those with birthday in particular month random sampling every sample in the dataset has an equal chance of being selected stratified sampling the data are divided into subgroups based on relevant features such as gender and age range sampling is conducted to ensure that every subgroup is accurately represented', 'in addition the following data planning aspects are important planning for the source of data acquisition planning for checking the quality of labelled data planning for coordinating the labelled and unlabelled data', 'approaches to sampling for the label result can include random sampling proportion of random samples is inspected stratified sampling proportion of random samples from each annotator is inspected full inspection all of the samples in the labelled dataset are inspected']"
0,28,0_supervised_semi_unsupervised_training,"['supervised', 'semi', 'unsupervised', 'training', 'unlabelled', 'labelled', 'learning', 'includes', 'acquisition', 'preparation']","['10.4 data acquisition when semi-supervised ml is conducted the data acquisition process for the supervised ml part and the unsupervised ml part is described in 7.4', '7.6.4 semi-supervised ml semi-supervised ml is hybrid of supervised and unsupervised learning and uses unlabelled training data in addition to labelled training data', '7.5.4 semi-supervised ml semi-supervised ml is hybrid of supervised and unsupervised learning and can make use of all of the elements described in 7.5.1']"
1,24,1_quality_assessment_measures_requirements,"['quality', 'assessment', 'measures', 'requirements', 'process', 'documentation', 'validation', 'principles', 'adherence', 'provenance']","['6.5 data quality process validation the data quality process validation activities and outcomes include activities assessment of whether the data meet requirements if the data do not meet requirements changes to some of the data quality processes are considered and implemented complete data provenance records if used outcomes documentation of the data quality assessment failure reports guidance on the improvement of the data quality processes approval for the use of the data for specified context by appropriate stakeholders', '6.3 data quality evaluation the data quality evaluation process activities and outcomes include activities apply data quality measures compare data quality measures results against established targets assess whether data requirements are met outcomes documentation of the differences and impact analysis between the results of data quality measures and established targets documentation of the data quality assessment', 'elements of data quality assessment should include at least the following statistical characteristics of the acquired data including an assessment of their impact on data requirements application of the data quality characteristics and data quality measures against the targets established in the data requirements process data quality characteristics and data quality measures are described in iso/iec 5259-2 documentation of the assessment process and results']"
2,23,2_labels_annotators_speech_annotator,"['labels', 'annotators', 'speech', 'annotator', 'annotation', 'video', 'audio', 'voice', 'recognition', 'data']","['labels should have well-defined semantics to ensure the labelling is understandable by both human annotators and automatic labelling systems', 'example for data labelling of speech data one layer can be used for the annotation of events time and space and speaker information the second layer can be used to annotate the contents of the speech signals the third layer can be used to annotate invalid speech signals', 'data can be labelled with speaker roles environmental scene labels multilingual labelling prosody labelling system labelling emotion labelling noise labelling etc']"
3,18,3_originator_planner_collector_roles,"['originator', 'planner', 'collector', 'roles', 'cloud', 'rights', 'users', 'entry', 'agreements', 'acquires']","['for example data originator can also be data holder', 'in some cases the roles of data originator data holder and data user are within the same organization', 'data holder can fulfil other roles such as data originator data collector and data engineer']"
4,15,4_process_analytics_ml_processes,"['process', 'analytics', 'ml', 'processes', 'processing', 'requirements', 'improvement', 'application', 'evaluation', 'approaches']","['data quality process for analytics 12.1 general for analytics the data quality process can be used to manage and support the quality of traditional data analysis approaches to discover useful information inform conclusions and support decision-making', 'figure relationship between data quality and data quality processes data quality process for ml 7.1 general the purpose of the data quality process described in this document is to provide guidance and good practices that organizations can use to ensure that data used for ml meets requirements', 'it also links processes that are mapped on the data life cycle model in iso/iec 5259-1. vi international standard artificial intelligence data quality for analytics and machine learning ml part data quality process framework scope this document establishes general common organizational approaches regardless of the type size or nature of the applying organization to ensure data quality for training and evaluation in analytics and machine learning ml']"
5,15,5_reinforcement_environment_driving_interaction,"['reinforcement', 'environment', 'driving', 'interaction', 'agent', 'sequence', 'example', 'signals', 'steps', 'training']","['11.5 data preparation 11.5.1 general process the data preparation process for reinforcement learning consists of four steps which should be specifically designed for different reinforcement learning tasks design the actions of the agent', 'data quality process for reinforcement learning 11.1 general for reinforcement learning data are generated through the learning process', '11.5.2 data recording based on the four steps of data preparation in 11.5.1 the agent can interact with the environment continuously in the process of reinforcement learning and the complete interaction process is recorded including actions performed by the agent the feedback received for each action for example in self-driving car scenario the feedback is the change of the road environment perceived by the sensors the reward for each step since the total reward of the agent from the environment varies greatly for different types of tasks']"
6,14,6_objectives_transaction_data_acquisition,"['objectives', 'transaction', 'data', 'acquisition', 'standardized', 'processes', 'improvement', 'transforming', 'insufficiencies', 'reusability']","['12.5.3 data transformation when the volume of data continues to increase the types of acquired data can vary greatly', 'in order to facilitate analysis the acquired data should be normalized through data cleaning data transformation and data aggregation', '12.4 data acquisition 12.4.1 general generally before the data preparation stage the analysis task goals should be defined to determine the source of the data acquisition and the attributes to be recorded']"
7,14,7_stage_information_roles_platform,"['stage', 'information', 'roles', 'platform', 'region', 'subcontracting', 'electropedia', 'ts', 'amendments', '22002']","['for further information see iso/iec tr 20547-2:2018 5.3.4', 'iso and iec maintain terminology databases for use in standardization at the following addresses iso online browsing platform available at iec electropedia available at 3.1 outsourcing subcontracting of an activity by an organization to an external organization source iso ts 22002-4:2013 3.14 modified definition revised', 'for further information see iso/iec tr 20547-2:2018 5.9.5']"
8,13,8_grouping_cluster_correlation_aggregation,"['grouping', 'cluster', 'correlation', 'aggregation', 'mining', 'clusters', 'clustering', 'database', 'visualization', 'patterns']","['data mining can include correlation analysis time series analysis and cluster analysis correlation analysis determines the correlation between different quantified events i.e', 'cluster analysis is the process of grouping items of data into different clusters so that items in each cluster are more similar to each other than to items in different clusters for example cluster documents into topics', 'after data grouping drawing frequency distribution table by calculating the frequency of data in each group is helpful to observe the underlying distribution of the data']"
9,13,9_cleaning_records_correcting_items,"['cleaning', 'records', 'correcting', 'items', 'storage', 'format', 'errors', 'case', 'inconsistent', 'incomplete']","['correcting date formats dropping records with incorrect data items from the dataset removing or filling empty or null data items e.g', 'in this case data cleaning can be used for checking the data consistency according to unified standard format dealing with the invalid data and missing data during storage and correcting the identifiable errors in the data storage file', 'duplicate records as result of combining data sets removing or correcting data records with incorrect data items e.g']"
10,13,10_metadata_field_schema_semantics,"['metadata', 'field', 'schema', 'semantics', 'dataset', 'directories', 'jpeg', 'structured', 'summarize', 'similarity']","['for complete definition meta-metadata can be provided in order to specify the metadata of field', 'examples of basic semantics can include the name of field the data type of field e.g', 'example schema for an image classification dataset can include file name field of type string label field of type integer data field of type bytes timestamp field of type datetime']"
11,13,11_dqpf_quality_model_management,"['dqpf', 'quality', 'model', 'management', 'data', 'outcomes', 'planning', 'map', 'measures', 'requirements']","['6.6 using the dqpf the dqpf provides additional detail for data quality management over the dlc model', 'the components of the dqpf include data quality planning establish the data quality management plans by analysing data quality requirements and data life cycle and determining the data quality management methods data quality evaluation measure and monitor data quality in the dlc model and provide results for the data quality plans data quality improvement implement data quality improvement processes e.g', 'key data quality management flow figure the relationship between the dlc model and the dqpf 6.2 data quality planning the data quality planning process activities and outcomes include activities analyse the data quality requirements from the stakeholders in the data life cycle model build the data quality model as described in iso/iec 5259-1 and iso/iec 5259-2 determine the appropriate data quality measures as described in iso/iec 5259-2 and target values for each measure outcomes data quality model data quality measures data quality measurements targets data quality work products such as results of data quality measurements failure reports data improvement and augmentation methods applied']"
12,12,12_agent_actions_environment_set,"['agent', 'actions', 'environment', 'set', 'accidents', 'rewards', 'optimality', 'learned', 'intervention', 'strategies']","['the data generated includes the state change of the environment the set of actions taken by the agent and the corresponding reward value', 'after the agent takes series of actions it determines the reward it received for this set of actions', 'in the process of interacting with the environment an agent constantly explores new actions so as to find the best decision-making strategy for the actions']"
13,12,13_pii_identification_identifiable_address,"['pii', 'identification', 'identifiable', 'address', 'anonymization', 'privacy', 'identifies', 'identity', 'identifiers', 'passport']","['for example personally identifiable information pii can be collected by data originator that identifies other individuals', 'examples of pii can include name address ip address location biometric information demographic information unique identifiers including passport numbers or credit card information', '7.5.10 data de-identification acquired data can contain personally identifiable information pii potentially compromising the privacy of the data subjects']"
14,12,14_labelling_figure_training_sample,"['labelling', 'figure', 'training', 'sample', 'stages', 'specification', 'manual', 'regression', 'notes', 'execution']","['iso/iec 23053:2022 8.3 describes data labelling as the process of assigning target variable to sample', 'instructions are used to clarify the label definitions and specify the label components label types and all operations used in the labelling tools or platforms', 'figure data labelling process 8.4.2 labelling specifications when implementing data labelling clear data labelling specification should include instructions labelling examples and labelling notes']"
15,12,15_tasks_labelling_annotators_organization,"['tasks', 'labelling', 'annotators', 'organization', 'crowdsourcing', 'risks', 'outsourcing', 'inspectors', 'roles', 'healthcare']","['even if crowdsourced labelling is used for low-risk tasks the organization should consider stringent data labelling result inspection', 'high-risk ml tasks can warrant an extensive inspection of the labelling results while label result inspection for low-risk ml task can consist of reviewing to of the labelled data', 'crowdsourcing labelling tasks where public exposure of the dataset is acceptable tasks where wide diversity of annotators is beneficial to the project tasks that are done by organizations with constrained development budgets tasks requiring large number of annotators to meet timeline requirements low-risk tasks']"
16,12,16_range_transforms_quartile_scale,"['range', 'transforms', 'quartile', 'scale', 'standardized', 'scaler', 'outliers', 'norm', 'minmax', 'deviation']","['examples of standardized transforms include the following standard scalar this transform creates dataset with mean of zero and standard deviation of one', 'maxabs scalar this transform scales each value to lie between and −1', 'robust scalar this transform scales each value to lie within the interquartile range i.e']"
17,10,17_layers_instances_mislabelled_labelling,"['layers', 'instances', 'mislabelled', 'labelling', 'examples', 'labels', 'label', 'probability', 'referencing', 'filtered']","['if this trained model predicts an instance as belonging to different label than given by its training data label then this instance has high probability of being mislabelled by the annotators', 'this allows for not only the use of layers that are not mutually consistent but also the use of alternative labels that employ different labelling schemes for the same object', 'layers of labelling over the object can coexist as uniform way of cross- referencing between layers']"
18,9,18_augmentation_noise_sample_injection,"['augmentation', 'noise', 'sample', 'injection', 'unbalanced', 'outward', 'flipping', 'rotated', 'resized', 'reverberation']","['typical methods for text data augmentation include entity replacement e.g', '7.5.9.4 data augmentation in some cases dataset used for ml training can not adequately represent the population undergoing analysis and prediction typical example is the under-sample or over-sample of specific labels with consequential unbalanced data', 'typical methods for image data augmentation include scaling the image is scaled outward and inward to create new image cropping section of the image is selected cropped and then resized to the original image size flipping the image is flipped horizontally vertically or both rotation the image can be rotated between 0° and 360° zooming noise injection']"
19,9,19_testing_preparation_models_production,"['testing', 'preparation', 'models', 'production', 'validation', 'training', 'requirements', 'development', 'performance', 'context']","['data that meets requirements is essential to training testing and validating ml models and to data analytics tasks', 'for ml if the training testing and validation data do not meet requirements the output of trained models can likewise fail to meet requirements', '7.5 data preparation 7.5.1 general the objective of the data preparation process is to get the data to state where it can successfully be used to develop an ml model and the performance of the model meets the organization requirements']"
20,8,20_tasks_organizational_labelling_task,"['tasks', 'organizational', 'labelling', 'task', 'table', 'knowledge', 'training', 'complexity', 'assignment', 'degree']","['generally the selection of the organizational approach for data labelling depends on the complexity and size of training data required domain knowledge and the degree of understanding of ml tasks by the teams assigned to implement data labelling', '8.4.6 labelling task assignment data labelling tasks should be assigned to different teams according to different organizational approaches', 'table applicability of different organizational approaches organizational approaches data labelling tasks internal labelling tasks that require extensive knowledge of the context of the ml task algorithms and data tasks that make use of highly sensitive data e.g']"
21,8,21_datasets_formats_component_information,"['datasets', 'formats', 'component', 'information', 'attributes', 'volume', 'metadata', 'partitions', 'refers', 'correspondences']","['it contains the information related to the volume and the locations of the scalar and block data as well as the information related to their correspondences statistics partitions data types and dimensions', 'to ensure that the datasets with different formats can be processed consistently by certain ml tool the datasets can be transformed serialized and stored in wrapped form where the metadata data samples and labels are well organized to improve data quality when the data are used', 'business and technical domains data formats data volume number of data categories sample examples data attributes statistical information related to data distributions bounding boxes segmentations key-points and files quality-related metadata e.g']"
22,8,22_categorical_values_transform_features,"['categorical', 'values', 'transform', 'features', 'binary', 'numeric', 'encoding', 'labelencoder', 'ordinalencoder', 'converted']","['get dummies this transform also converts categorical values to an array of features and binary indicators', 'onehotencoding this transform converts set of categorical values to an array with categorical labels presented as features and binary values that indicate whether the entry fits under given feature', 'transforms for encoding categorical data include the following ordinalencoder this transform converts set of categorical values to an ordinal set of integers i.e']"
23,8,23_values_missing_imputation_imputing,"['values', 'missing', 'imputation', 'imputing', 'median', 'iterative', 'replace', 'nan', 'column', 'blanks']","['tabular data in comma-separated values one method of dealing with missing values is to delete the entire row or column with missing value', 'transforms for statistical imputation include simple imputer this transform can be programmed to replace missing values with mean median most-frequent or constant value', 'iterative imputer this multivariate imputation transform approximates replacements for missing values by analysing the values for all features in the dataset in an iterative fashion']"

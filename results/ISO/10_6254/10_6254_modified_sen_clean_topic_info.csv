Topic,Count,Name,Representation,Representative_Docs
-1,273,-1_model_methods_information_data,"['model', 'methods', 'information', 'data', 'explainability', 'inputs', 'case', 'scope', 'behaviour', 'processing']","['approaches and methods to explainability 9.1 general this clause provides an overview of methods to achieve explainability along each of the approaches described in 8.4 empirical analysis methods post hoc methods inherently interpretable components as well as architecture- and task-driven methods', '9.3 post hoc methods 9.3.1 local 9.3.1.1 general this subclause describes methods to achieve local explainability using post hoc analysis tools', 'the properties are denoted using abbreviations from 8.1 properties related to explanation needs are indicated with seven columns for audience expertise with possible values ai ai specialists domain experts laypersons for frame activity with possible values development familiarization decision validation posteriori analysis for scope with possible values local global for completeness with possible values nc near-complete cp core partial partial for depth with possible values static di directed dy dynamic for reasoning path with possible values sf system-faithful -sf not system-faithful ha human- aligned -ha not human-aligned for explicitness with possible values implicit explicit']"
0,80,0_explainability_evaluation_methods_deployment,"['explainability', 'evaluation', 'methods', 'deployment', 'stakeholders', 'shortcomings', 'developers', 'objectives', 'technical', 'interpretability']","['the practical impact on the development process depends on which approach the chosen methods belong to if using an explainability method based on inherently interpretable components see 8.4.4 or architecture-driven and task-driven methods see 8.4.5 the implementation of the explainability method is the same process as the development of the ai system itself', 'vi technical specification information technology artificial intelligence objectives and approaches for explainability and interpretability of machine learning ml models and artificial intelligence ai systems scope this document describes approaches and methods that can be used to achieve explainability objectives of stakeholders with regard to machine learning ml models and artificial intelligence ai systems behaviours outputs and results', '3.12 application programming interface api boundary across which software application uses facilities of programming languages to invoke software services source iso/iec 13522-6:1998 3.3 3.13 backpropagation neural network training method that uses the error at the output layer to adjust and optimise the weights for the connections from the successive previous layers source iso/iec 23053:2022 3.2.1 3.14 classification model machine learning machine learning model whose expected output for given input is one or more classes source iso/iec 23053:2022 3.1.1 3.15 closed box black box access property of an ai system 3.4 or model within an ai system whereby only its outputs can be obtained programmatically 3.16 opaque box black box explainability property of an ai system 3.4 or model within an ai system whereby it does not offer intrinsic interpretability 3.17 3.17 intrinsic interpretability inherent interpretability property of an ai model that holds its criteria and decision process 3.21 in an intelligible way in its structure or content note to entry intrinsic interpretability is not limited to access only but also implies an ability to understand the provided information']"
1,57,1_users_explanations_behaviour_response,"['users', 'explanations', 'behaviour', 'response', 'acceptance', 'example', 'risk', 'empirical', 'stimuli', 'outcomes']","['in such cases persuasive explanations can be financially beneficial for one stakeholder group due to their persuasive nature ai producers can be incentivized to display explanations even if inaccurate to encourage ai user acceptance', 'example scenarios where ai users seek explainability include judge who needs to decide whether to use an automated pretrial bail risk assessment system for upcoming cases or the same judge who for given case wants to know the extent to which the generated risk score is reliable medical doctor considering to overrule the recommendation of an automated diagnosis tool because the proposed diagnostic is atypical the medical doctor who announces serious illness to patient or biologist who can discover new causes for known illnesses thanks to explainable analysis of health data', 'ai users and ai subjects can be disadvantaged due to their reliance on ai producers to provide appropriate accurate and timely explanations']"
2,44,2_trees_interpretability_model_lists,"['trees', 'interpretability', 'model', 'lists', 'margin', 'node', 'bayesian', 'rules', 'algorithm', 'criterion']","['decision trees are based on repeated if-then-else statements that are organized in tree structure', 'tree regularization is common choice where the simpler model is decision tree but it can also be done with decision lists or other models', 'soft decision trees combine the legibility of hierarchical decisions from decision trees and the expressivity from neural networks by learning filter single-layer perceptron for each node of the tree']"
3,40,3_stakeholders_objectives_stakeholder_churn,"['stakeholders', 'objectives', 'stakeholder', 'churn', 'expectations', 'action', 'regulatory', 'mitigation', 'discussed', 'field']","['stakeholders objectives 6.1 general xai is broad field and stakeholders can have very different reasons to seek explainability', 'the methods to determine the explanation and the way to present it to stakeholders are determined by available technical means as well as by the specific objectives and expertise of the stakeholders', 'depending on the objectives of stakeholders this explanation can thus be appropriate to address the explanation needs']"
4,39,4_explainability_needs_properties_approaches,"['explainability', 'needs', 'properties', 'approaches', 'taxonomy', 'component', 'implementation', 'research', 'subclauses', 'depth']","['property taxonomy of explainability methods and approaches 8.1 general the following subclauses describe taxonomy of explainability methods and approaches based on multiple properties', '7.4.2.2 alignment with explanation needs the adequacy between the explanation needs and the actual properties of the explainability component see 8.2 is quality criterion by itself', '8.2 properties of explanation needs 8.2.1 general the following subclauses describe taxonomy of explanation needs depending on the type of explanation needed in particular use case and the needed properties']"
5,37,5_prototypes_dataset_distribution_samples,"['prototypes', 'dataset', 'distribution', 'samples', 'confidence', 'selection', 'clustering', 'outliers', 'automated', 'mmd']","['prototypes are data instances which together relay the essence of the whole dataset', 'mmd-critic extends the prototype selection approach by considering data distribution and also identifying criticisms which are data instances poorly represented by the set of prototypes e.g', 'an alternative way to understand dataset is through prototypes samples that relay the essence of dataset and criticisms samples that are outliers']"
6,36,6_pixels_detection_images_visual,"['pixels', 'detection', 'images', 'visual', 'giraffe', 'shoulder', 'classification', 'texture', 'aerial', 'cyclists']","['if object detection is explicitly performed during the processing it enables easier identification that the cause of given road accident is for instance that given pedestrian has not been detected perhaps for an obvious reason that auditors can then pinpoint more easily or that motorbike has been misidentified as car and therefore its possible movements misinterpreted', 'for instance in the automated steering example object detection can be performed using end-to-end deep learning from raw images to the detected objects', 'meanwhile lime is prone to select arbitrary features regardless of their meaningfulness to humans for instance when applying lime on super pixels in the case of image classification the selected super pixels can actually be brown spots on the giraffe skin']"
7,27,7_summarizes_properties_tables_subclauses,"['summarizes', 'properties', 'tables', 'subclauses', 'diversity', 'inventory', 'groupings', 'implement', 'hierarchy', 'concrete']","['table summarizes the properties of the methods described in this subclause', 'table summarizes the properties of the methods described in this subclause', 'table summarizes the properties of the methods described in this subclause']"
8,26,8_gradients_vector_features_umap,"['gradients', 'vector', 'features', 'umap', 'sne', 'propagation', 'pca', 'inputs', 'tcav', 'weights']","['vector is built for each concept by collecting inputs that are representative of that concept training linear classifier to distinguish between their representation and the representation of random counterexamples and retaining the weight vector corresponding to the decision boundary', 'linear regression computes the output as weighted sum of input features with weights usually optimized through the least squares method', 'the tcav score for class and concept is computed as the fraction of example inputs in that class that are positively influenced by that concept in the sense that the output gradient with respect to the input representation has positive dot product with the concept vector']"
9,25,9_attention_visualization_weights_embeddings,"['attention', 'visualization', 'weights', 'embeddings', 'conveys', 'explanations', 'neural', 'importance', 'similarities', 'illustrates']","['figure b.7 example of attention visualization by contrast deep shap is an example of method that produces directed explanations as illustrated in figure b.8', 'figure b.12 use of attention visualization to understand incorrect predictions figure b.13 illustrates how feature visualization based on activation maximization can produce implicit explanations', 'attention visualization is specific to attention-based neural networks']"
10,25,10_input_focusing_reveals_localization,"['input', 'focusing', 'reveals', 'localization', 'compositionality', 'enhance', 'salient', 'blur', 'anisotropy', 'smoothness']","['conversely if the reduced input appeared irrelevant such as focusing on commas the developer would know to investigate potential overfitting', 'repeating probing experiments reveals which information the model is focusing on and where in the model it is detected', 'it reveals the aspects of the input that this part of the model is focusing on']"
11,25,11_feature_contribution_plots_importance,"['feature', 'contribution', 'plots', 'importance', 'prediction', 'pdps', 'marginal', 'dependence', 'anova', 'averaging']","['aggregation strategies include averaging feature contribution over data instances to compute its importance drawing summary plot of contributions for each data instance or dependence plots of the contribution with respect to feature value as well as clustering data instances according to the similarity of their feature contributions', 'pdps partial dependence plots show the effect of given feature variations by computing the average prediction obtained over all data instances when that feature is set to particular value then varying that value', 'shapley values measure the marginal contribution of feature to the outcome compared to the average outcome in absence of that feature computing the average both over all subsets of features and over all data instances corresponding to that subset']"
12,23,12_neuron_neurons_concept_activations,"['neuron', 'neurons', 'concept', 'activations', 'patterns', 'dataset', 'clustering', 'segmentation', 'neural', 'input']","['rnnvis co-clusters neurons and words based on neuron sensitivity to input words to display the association strength of input words with clusters of neurons and in turn with clusters of other words', 'activation maximization builds for given neuron synthetic input that maximizes the activation of that neuron', 'activation patterns of individual neurons over dataset can be correlated with various property values of the input to identify the property that that neuron represents the most']"
13,22,13_neural_networks_distillation_layer,"['neural', 'networks', 'distillation', 'layer', 'models', 'regularization', 'compositional', 'deeper', 'posterior', 'pipelines']","['neural networks', 'at given layer of neural network', 'neural networks only but not one neural network architecture in particular']"
14,22,14_transparency_trustworthiness_technology_organization,"['transparency', 'trustworthiness', 'technology', 'organization', 'accountability', 'definitions', 'retirement', 'verification', 'stages', 'stakeholders']","['various transparency elements e.g', '3.3 transparency system property of system that appropriate information about the system is communicated to relevant stakeholders 3.1 note to entry appropriate information for system transparency can include aspects such as features performance limitations components procedures measures design goals design choices and assumptions data sources and labelling protocols', 'in contrast to transparency of an organization and system transparency ai system explainability means to be able to describe in human understandable terms an output of an ai system or an ml model to stakeholders']"
15,22,15_forms_explanations_graphs_textual,"['forms', 'explanations', 'graphs', 'textual', 'examples', 'grammar', 'answering', 'structured', 'inputs', 'phrasings']","[""example-based explanations can be of various forms in practice as they have the same form of the system 's inputs"", 'however more direct approach is to cast instead this user need as functional need for passage retrieval model without consideration of explainability and to complement it with user interface that suitably displays the retrieved passage natural language text holding the answer and link to the document', 'for instance consider question answering system to which the user inputs natural language query and expects natural language answer to the question asked']"
16,19,16_auxiliary_output_annotated_selection,"['auxiliary', 'output', 'annotated', 'selection', 'production', 'supervision', 'learns', 'task', 'hand', 'mimic']","['9.5.7 rationale generation as auxiliary output explainability of the ai system decision process can be achieved by automatically generating rationale as an auxiliary output next to the decision', 'producing human-aligned explanations is the typical purpose of methods involving rationale generator that has been learned on training data with manually annotated rationales human subjects indicate what would be their own rationale and the model learns to mimic that so what it actually learns is to produce human-aligned rationales that can convince human beings of the appropriateness of the decision itself independently of what internally happened in the system', 'there are various ways to integrate the rationale generation part into the model for instance concatenating the decision output with the rationale output or using multi-task architectures where common internal state is reused separately by the decision module and the rationale generation module']"
17,18,17_security_privacy_trust_concerns,"['security', 'privacy', 'trust', 'concerns', 'messages', 'persuasive', 'displays', 'limitations', 'fatigue', 'distract']","['as addressed in c.3.2 users and consumers of explainability displays can be convinced to rely on persuasive explanations that can be maliciously crafted by display providers given that providers of explainability displays can be incentivized to provide explanations to users even if inaccurate', 'it is relevant to ensure that explanation messages do not suffer the same fate as security dialogues which have been found in usable security research to cause pop-up fatigue leading to end users dismissing and ignoring such messages', 'c.4 concerns and limitations related to security and privacy explanations can also present security and privacy risks to several stakeholders including providers producers customers partners and subjects']"
18,15,18_explanations_implicit_evidence_cognitive,"['explanations', 'implicit', 'evidence', 'cognitive', 'legibility', 'lawyer', 'efficacy', 'efficiency', 'misuses', 'misattribution']","['conciseness is one way to achieve legibility as an explanation that conveys large amount of information can be hard to read however visualization and ergonomic displays can also help in making the explanations more legible', 'implicit explanations are often technically more feasible than explicit explanations since they are less constrained and challenging but processing implicit explanations can be more time-consuming due to the cognitive effort that the human has to perform', 'this illustrates why explicit explanations can be more relevant than implicit ones in contexts such as gathering evidence for legal proceedings because evidence is hard to leverage if the judge attorney and lawyer do not read it the same way']"
19,15,19_feature_features_readable_likelihood,"['feature', 'features', 'readable', 'likelihood', 'expertise', 'engineering', 'documentation', 'multinomial', 'wording', 'number']","['the defined features should be described in plain words in an available documentation together with the intuition or expert knowledge based on which the domain expert has decided to define given feature', '3.11 feature-based explanation explanation 3.27 of model behaviour 3.22 based on input features 3.7 note to entry for instance measure of how much each input feature contributes to model output for given data point is an example of feature-based explanation', 'when available domain expertise is limited or to facilitate the process of feature engineering it is also possible to define larger number of features using feature templates and to automatically select the most relevant ones using feature selection methods']"
20,14,20_data_audio_features_accuracy,"['data', 'audio', 'features', 'accuracy', 'inputs', 'unstructured', 'categorical', 'consumers', 'tabular', 'snippets']","['tabular data for which features are readily defined and unstructured input data e.g', 'whereas replacing the input with human-defined features can induce loss of accuracy for models such as end-to-end feature-less neural networks adding that information on top of the input can instead be beneficial to accuracy which overcomes the accuracy-interpretability trade-off', 'likewise if input data consists of audio data then the explanations can include snippets of audio or certain characteristics of audio data extracted from input data']"
21,14,21_comprehensiveness_displays_presented_metrics,"['comprehensiveness', 'displays', 'presented', 'metrics', 'information', 'view', 'priority', 'forefront', 'presentation', 'resolutions']","['within the display', 'c.5.2 display comprehensiveness an important consideration for ai explainability displays is the comprehensiveness of the display i.e', 'excessive display comprehensiveness the following issues can occur stakeholders are presented with an overwhelming amount of information that they can not digest leading to lack of clarity and ignorance of essential elements of the display']"
22,13,22_stage_inception_validation_mapping,"['stage', 'inception', 'validation', 'mapping', 'development', 'considerations', 'objectives', 'completeness', 'assessment', 'settings']","['such cases are identified during inception when mapping the explainability objectives to practical properties in particular if the frame activity see 8.2.3 is decision validation or posteriori analysis', 'such cases are identified during inception when mapping the explainability objectives to practical properties in particular if the frame activity see 8.2.3 is development', 'such cases are identified during inception when mapping the explainability objectives to practical properties in particular if the frame activity see 8.2.3 is development or familiarization']"
23,13,23_stakeholder_domain_developers_external,"['stakeholder', 'domain', 'developers', 'external', 'roles', 'expertise', 'audiences', 'laypersons', 'deployers', 'collaborate']","['this can concern audiences composed of laypersons or domain experts see 8.2.2 depending on the targeted ai users', 'this can concern audiences composed of laypersons or domain experts see 8.2.2 depending on the targeted ai users but it can also concern other stakeholders beyond ai users for instance ai specialists can be involved to help investigating an incident', 'this can concern audiences of various expertise see 8.2.2 as ai developers are not the only stakeholders that are involved during development and they can also collaborate with domain experts or laypersons to get external feedback']"
24,12,24_perceptron_learning_examples_lazy,"['perceptron', 'learning', 'examples', 'lazy', 'instrumental', 'supervised', 'instances', 'models', 'slicing', 'describes']","['9.3.1.6 data influence this subclause describes methods based on quantifying how examples seen at training time influenced the processing of given input', '9.3.2.6 data analysis this subclause describes methods that identify key instances in dataset either as the training instances that affected the model the most or as examples that best convey the overall behaviour of the model', 'measuring overlaps between training and test data to assess information leakage or checking the quality of the labels in the training data are other ways to understand the training process and gain insights on the resulting trained models']"
25,12,25_note_entry_decisions_justification,"['note', 'entry', 'decisions', 'justification', 'outcomes', 'process', '24', 'engineered', 'future', 'information']","['3.22 behaviour ai system any observable effect of given decision process 3.21 such as particular decision 3.18 the preferences made among different outcomes 3.19 relationship among multiple decisions or statistical property of the complete set of decisions made by the ai system 3.4 including future decisions note to entry depending on the design of the ai system the behaviour of the ai system can be attributed to the behaviour of the ai system models or to their interplay', '3.26 justification piece of information or the analysis made of that information that is sufficient to choose given decision 3.18 among the possible outcomes 3.19 note to entry justification identifies causes relevant to given decision without assumption on the set of causes that have affected the decision process of the ai system 3.4', '3.23 factor element property or other characteristic that is considered during the decision process 3.21 and can have an effect on the chosen decision 3.18 3.24 cause any type of circumstance that can lead to given decision 3.18 including for instance the presence absence or value of factor 3.23 but also the analysis made of that factor its similarity or interaction with other factors or the presence or absence of given step or criterion in the decision process 3.21 3.25 rationale piece of information or the analysis made of that information based on which decisions 3.18 are made note to entry rationale provided for single decision identifies one or more causes as having affected the decision process 3.21 of the ai system 3.4 when choosing that particular decision']"
26,12,26_domain_audience_expert_technical,"['domain', 'audience', 'expert', 'technical', 'specialists', 'expertise', 'formalizations', 'advisors', 'design', 'topic']","['8.2.2 expertise profile of the targeted audience needs for explainability differ with respect to the expertise profile of the audience for which an interpretation or explanation is needed ai specialists domain experts or laypersons with no specific background in ai or the particular domain', 'for such an audience explanations are often more effective if the explanations rely on domain-specific elements which enable the expert to connect the explanations with their own domain knowledge', 'for such non- expert audience explanations should make limited use of both ai-related technical elements as for domain experts and domain-specific elements as for ai specialists']"
27,12,27_yellow_shirt_syntactic_words,"['yellow', 'shirt', 'syntactic', 'words', 'model', 'textual', 'structured', 'text', 'language', 'lexical']","['the form property is indicated with single column with possible values numeric visual textual structured example-based interactive exploration tool properties related to technical constraints are indicated with three columns for genericity with possible values ma model-agnostic fs family-specific ms model-specific ss system-specific', 'for instance given the input sentence this shirt is red not yellow answering no to the question is this yellow shirt can be done either based on the presence of the word red on not yellow or on red not yellow', 'for instance in computer vision concepts are often materialized as objects or textures in pictures contiguous sets of features in 2d layout whereas in natural language processing concepts can be either set of words with similar meanings set of words sharing the same grammatical property values or chunks of consecutive words whose boundaries respect some linguistic criteria']"
28,11,28_permission_authors_reproduced_source,"['permission', 'authors', 'reproduced', 'source', 'edition', 'references', 'cited', 'amendments', 'multimodal', 'figure']","['source reference reproduced with the permission of the authors', 'source reference reproduced with the permission of the authors', 'source reference reproduced with the permission of the authors']"
29,11,29_disentangled_autoencoder_dip_vae,"['disentangled', 'autoencoder', 'dip', 'vae', 'variational', 'examples', 'pretrained', 'gans', 'naturalness', 'disentanglement']","['in this case some approaches are used that take the given features and learn new representation that is disentangled in such way that the resulting features are understandable', 'the dip-vae disentangled inferred prior variational autoencoder algorithm is just one of such examples', 'the disentangled inferred prior variational autoencoder dip-vae algorithm is just one of such examples']"
30,11,30_3d_evolves_landscapes_clusters,"['3d', 'evolves', 'landscapes', 'clusters', 'visualizations', 'training', 'layers', 'gradients', 'cell', 'rnns']","['they do so by computing activations for pieces of all training instances projecting them into 2d layout and then synthetizing inputs corresponding to each cell mean activation within 2d grid', 'loss surfaces are 3d visualizations of how the model loss evolves in the neighbourhood of given input meaning its surface in the input space', 'loss landscapes are 3d visualizations of how the model training loss evolves in the neighbourhood of given trained model meaning its surface in the parameter space']"
31,10,31_challenge_datasets_patterns_schemas,"['challenge', 'datasets', 'patterns', 'schemas', 'winograd', 'hierarchically', 'failures', 'configurations', 'targeting', 'oriented']","['challenge sets are not designed to target specific model or system but are designed generically for given task or domain and can be reused and shared across stakeholders of various ai systems', '9.2.4 analysis-oriented datasets challenge sets are datasets specifically designed to enable analysis and understanding of an ai system behaviour along given characteristic', 'specific datasets designed to reveal one specific characteristic of an ai system decision process are referred to as challenge sets']"
32,10,32_apis_tooling_deployment_considerations,"['apis', 'tooling', 'deployment', 'considerations', 'plugins', 'frontend', 'custom', 'toolkit', 'interact', 'incorporate']","['c.5.4 control mechanisms it is useful that explainability displays also incorporate control mechanisms for stakeholders to adequately interact with and control the amount of information present in the interface through apis or the front-end toolkit', 'in that case care needs to be taken that explainability-related software is integrated in way that it has access to the necessary apis and to the appropriate display tooling see 8.5.3 and 8.5.4', 'c.5 concerns and limitations related to explanation communication c.5.1 communication through api if the explanation is communicated through apis these apis can supplement low-code or no-code frontend tooling as well']"
33,9,33_taxonomy_methodology_considerations_thematically,"['taxonomy', 'methodology', 'considerations', 'thematically', 'normative', 'constitutes', 'guides', 'formalize', 'landscape', 'facilitating']","['for legibility and ease of use of the document within each approach the methods are organized thematically so that methods in the same group share the same concepts and general methodology and can be discussed together', 'guidance on that process is offered in clauses and provide further technical material taxonomy of properties for needs assessment and corresponding landscape of methods to support that methodology', 'the taxonomy described in 8.2 offers rigorous methodology to formalize those properties by considering the various aspects of the context and expectations and facilitating further association of the needs with practical methods']"
34,9,34_representations_rsa_similarity_conductance,"['representations', 'rsa', 'similarity', 'conductance', 'encoding', 'variations', 'semantics', 'describes', 'generative', 'inferring']","['9.3.2.5 representation-based methods this subclause describes methods that make various uses of internal representations to extrapolate the inner workings of the model either by inferring their semantics or comparing them with each other', 'the rsa-regress variant extends it with linear regression to quantify how predictable the input similarities are when given the representation similarities', 'rsa representation similarity analysis computes for given internal representation the correlation between the similarities among inputs and the similarities among their representations']"
35,9,35_tools_interactive_exploration_dashboards,"['tools', 'interactive', 'exploration', 'dashboards', 'probes', 'experimentation', 'bundled', 'specification', 'designing', 'visualizations']","['in particular it can be beneficial to specify at the model level task that differs from the task as viewed by the users', 'a.6 differences with task specifications explainability is sometimes confused with the need for careful specification of the task to address', 'such tools often appear as dashboards aggregating multiple advanced visualizations metrics and probes sometimes bundled with experimentation functionalities']"
36,9,36_masked_importance_randomized_map,"['masked', 'importance', 'randomized', 'map', 'sampling', 'pixel', 'losses', 'riseplus', 'dimension', 'empirically']","[""rise randomized input sampling for explanation generates an importance map to indicate the saliency of each pixel for model 's prediction"", 'rise operates on black-box models to empirically estimate importance by probing the model with multiple versions of randomly masked input image and eventually obtain corresponding outputs', 'representation erasure can also produce importance scores for either features or dimensions of internal representation by averaging over the dataset the shift in confidence towards the correct output when that dimension is masked']"
37,9,37_notations_details_particular_,"['notations', 'details', 'particular', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['see 9.1 for notations', 'see 9.1 for notations', 'see 9.1 for notations']"
38,9,38_shap_attribution_game_models,"['shap', 'attribution', 'game', 'models', 'hierarchy', 'aggregated', 'deeplift', 'assessing', 'features', 'values']","['acd agglomerative contextual decomposition applies hierarchical clustering on top of contribution decomposition to draw tree-shaped hierarchy of the attribution scores for individual features but also for combined features up until the full input', 'for neural networks deep shap computes shap on smaller components of the network and then propagates the values using deeplift', 'the general shap method can be applied on opaque box models while deep shap usually requires full access to the model internals']"
39,9,39_memory_knowledge_net_tb,"['memory', 'knowledge', 'net', 'tb', 'embedding', 'neural', 'turing', 'subgraphs', 'representations', 'querying']","['for example using tb-net as an interpretable white box recommendation model or system based on knowledge graphs', 'for instance in neural network this can be done by means of an intermediate layer that identifies the corresponding information selects from memory component relevant embedding to represent that information and adds it back in the network to enrich the input', 'example methods to integrate such components in the decision process of neural network are by means of attention on embeddings of nodes or edges in the back-end knowledge graph by querying the knowledge component and embedding the results as additional input to neural layer or by explicitly matching objects concepts or properties observed in the input with entries in the knowledge component']"
40,9,40_paths_causal_reasoning_bayesian,"['paths', 'causal', 'reasoning', 'bayesian', 'causality', 'variables', 'networks', 'incompatible', 'causally', 'frequent']","['whereas in the previous toy example all options appear equivalent there are also cases where not all reasoning paths are equally appropriate from the human point of view', 'structural causal models are variant of bayesian networks where the relationship from one variable to another is modelled by dedicated causal functions so that questions why and what if are better addressed and distinguished than with pure statistics', '9.4.3.3 causal methods this subclause describes methods to build models with explicit reasoning paths where each processing step exhibits causality']"
41,9,41_logic_logical_formulas_programming,"['logic', 'logical', 'formulas', 'programming', 'logics', 'symbolic', 'labelled', 'knowledge', 'solvers', 'inductive']","['inductive logic programming learns first-order logical rules from labelled training data', 'expert systems rely on the combination of set of facts logic formulae set of rules logic implications and an inference engine to draw conclusions based on observations', 'markov logic networks draw connections between first-order logic formulas with weights and probabilities to benefit from explicitly inputting the observed facts as well as clearly outputting all facts that are deemed to be true by the model']"
42,9,42_training_weighted_influence_instances,"['training', 'weighted', 'influence', 'instances', 'negative', 'prediction', 'dfbeta', 'nearest', 'algorithm', 'neighbours']","['influence functions measure the contribution of each training instance for given prediction by analysing the effect on the output loss when upweighting that training instance', 'locally weighted learning extends k-nn by training local model based on training samples in the input neighbourhood weighted according to their distance', 'tracin computes the influence of given training instance on particular prediction by summing the changes in that input loss whenever the training instance was used throughout the training process']"
43,8,43_ease_legal_explainability_limitations,"['ease', 'legal', 'explainability', 'limitations', 'innovation', 'objectives', 'investigations', 'actionability', 'aspect', 'boundaries']","['6.11.3 other authorities other authorities can seek explainability for purposes such as investigations and legal proceedings', 'for policy makers understanding the capabilities and limitations of different explainability methods can help to develop effective policy frameworks that best address societal needs while promoting innovation', '6.11 relevant authorities 6.11.1 policy makers no explainability objectives have been identified']"
44,8,44_trends_nn_properties_constraints,"['trends', 'nn', 'properties', 'constraints', 'models', 'technical', 'taxonomy', 'needs', 'bayesian', 'processing']","['table properties of architecture- and task-driven methods along the taxonomy in clause methods properties explanation needs form technical constraints informative features ai+d+l any l+g sf ha ma m+is rich and auxiliary inputs ai+d+l any l+g sf ha ma m+is multi-step processing ai+d+l any l+g cp dy sf ha ma ir as model-internal pipeline ai d+a l+g cp dy sf ha fs m+is a+e with separate knowledge ai+d+l any l+g cp dy sf ha ms m+is rich outputs ai+d+l any l+g cp di sf ha ma ir a+e rationale-based processing d+l any l+g cp dy sf ha ms ir+is rationale generation as auxiliary output d+l f+v cp dy -sf fs with supervision from human rationales d+l f+v cp dy ha fs 9.5.2 informative features explainability of the ai system decision process can be improved by increasing the informativity of the input representation on which the model is applied', 'table properties of local post hoc methods along the taxonomy in clause methods properties explanation needs form technical constraints surrogate models trends -sf ma io lime lemna ai+d+l any cp ma io anchors ai+d+l ma io lore ai+d+l ma io attribution methods trends representation erasure occlusion di rise riseplus vanilla gradient smooth- grad di multirnnexplorer ms integrated gradients internal influence conduct- ance integrated hessians di nn cam grad-cam di ms lrp di nn deeplift nn shap di ma deep shap di nn methods properties explanation needs form technical constraints prediction difference analysis acd dy attention visualization -sf bertviz ai ms attention flow sf rnnbow ms perturbation-based trends meaningful perturbation input reduction ai+d+l input erasure ai+d+l mutual information ai+d+l ma gnnexplainer ms counterfactual explana- tions ai+d+l cem cem-maf ai+d+l loss surfaces ai representation analysis trends code inversion caricaturization activation grid nn rnnvis ms lstmvis ms data influence trends ml influence functions representer points tracin 9.3.1.2 surrogate models this subclause describes methods based on building new model that approximates the behaviour of the initial model in the neighbourhood of given input', 'table properties of models with explicit knowledge along the taxonomy in clause methods properties explanation needs form technical constraints rule-based models trends ai+d+l any l+g nc dy sf fs decision trees ai+d+l any l+g nc dy sf ms decision lists bayesian rule lists ai+d+l any l+g nc dy sf ms decision sets ai+d+l any l+g nc dy sf ms logics-based models and symbolic learning trends ai+d+l any l+g nc dy sf ha fs m+is expert systems ai+d+l any l+g nc dy sf ha fs m+is formal concept analysis ai+d+l any l+g nc dy sf ms inductive logic program- ming ai+d+l any l+g nc dy sf ha ms m+is probabilistic extensions of logic programming ai+d+l any l+g cp dy sf n+s fs m+is markov logic networks ai+d+l any l+g cp dy sf n+s ms m+is knowledge-infused models trends ai+d+l d+f+a l+g dy sf ma handcrafted heuristics information-theoretic meas- ures ai+d d+f+a l+g di sf ma m+is rule knowledge distil- lation d+l d+f di sf fs logical constraints over outputs d+f+a l+g di sf ma m+is activation regularization against target patterns ai d+f+a l+g dy sf fs m+is a+e 9.4.4.2 rule-based models this subclause describes methods that process the input based on explicit imperative rules']"
45,8,45_decision_insignificant_misinterpretations_perceptible,"['decision', 'insignificant', 'misinterpretations', 'perceptible', 'semantics', 'intuition', 'complexity', 'modification', 'mental', 'theoretic']","['this alleviates the mental complexity of understanding the impact of an earlier rule on the exact scope of later criterion', 'these explanations convey decision processes that are comparable to human decision processes and are based on human-known concepts and human-perceptible observations in the inputs', 'this intuition follows from how humans explain their decisions an insignificant change in the decision process does not change the human explanation']"
46,8,46_ambiguity_contextualize_forwarded_layperson,"['ambiguity', 'contextualize', 'forwarded', 'layperson', 'conveying', 'actionability', 'includes', 'forms', 'interpretation', 'information']","['actionability can take various forms depending on the use case', 'usually it includes at least conveying specific information', 'they can be easily forwarded to third parties and can be utilized without any further interpretation or any ambiguity about their contents and meaning']"
47,8,47_loan_cans_application_disease,"['loan', 'cans', 'application', 'disease', 'number', 'sardine', 'fish', 'anchovies', 'factors', 'feature']","['based on that information and the high feature number of anchovy cans bought in week the model has inferred that you eat anchovies because you have to', ""b.2.5 depth considering scenario where loan application gets rejected based on an ai system decision the three following explanations can apply to the same case but with various degrees of depth static explanation `` when deciding to reject your loan application the model has looked lot at the features number of anchovy cans bought in week and number of sardine cans bought in week ''"", 'directed explanation the model has decided to reject your loan application because the feature number of anchovy cans bought in week was high and the feature number of sardine cans bought in week was low. dynamic explanation there is rare degenerative disease for which the recommended treatment is to eat lot of anchovies']"
48,8,48_faithfulness_reasoning_human_aligned,"['faithfulness', 'reasoning', 'human', 'aligned', 'surrogate', 'method', 'intuitions', 'explanations', 'systematically', 'enforcing']","['8.2.7 reasoning path needs for explainability differ with respect to whether system-faithful or human-aligned explanations are needed', 'human-aligned explanations provide reasoning path from the inputs to the outputs one reasoning path among others not necessarily the system one that suitably justifies the decision to human', 'in addition some explainability methods are neither system-faithful nor human-aligned since they are focused on producing any valid reasoning path from the inputs to the produced outputs without enforcing its adequacy with either the system inner workings or with human intuitions']"
49,8,49_confirmation_bias_expectations_session,"['confirmation', 'bias', 'expectations', 'session', 'misinterpretation', 'cognition', 'entangled', 'interpreted', 'interpretations', 'influence']","['there is confirmation bias that leads people to interpret what they expect', 'c.2.3 over-interpretation it can be close to confirmation bias but here there are no prior expectations that influence interpretation', 'c.2 concerns and limitations related to human cognitive biases c.2.1 general many things can be wrong between an explanation and its understanding and thus its interpretation']"

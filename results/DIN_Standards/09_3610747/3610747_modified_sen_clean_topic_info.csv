Topic,Count,Name,Representation,Representative_Docs
-1,169,-1_kg_knowledge_web_embeddings,"['kg', 'knowledge', 'web', 'embeddings', 'retrieval', 'semantic', 'context', 'llms', 'task', 'queries']","['when it comes to integrating kg knowledge directly into the training process using knowledge embeddings current techniques modify the model architecture in three main ways inserting knowledge encoder after the transformer encoder to fuse text embeddings with knowledge embeddings after the initial text encoding allowing the model to use both textual and inserting knowledge encoding layer in between the layers of the transformer block to enable the llm to process knowledge from the kg adding an adapter that is trained independently of the llm and is easier to train because it has its own set of parameters', 'knowledge encoder layer layer that is designed to process and encode contextual knowledge within large language model output layer layer whose artificial neurons send signals to an external source modified admitted term decision layer and notes to entry have been deleted embedding in natural language processing technique for converting words or phrases into numerical vectors note to entry embeddings capture the semantic relationships and contextual of the text that is being processed and allow machines to understand and process language more effectively', 'source din question answering qa task of determining the most accurate answer to question provided in natural language source din modified appropriate has been replaced with accurate extraction task of extracting structured from unstructured natural language text din spec sentiment analysis task of identifying and quantifying the emotional tone of natural language text often with respect to specific subject or topic text summarisation task of distilling the most important from one or more texts to produce condensed version for particular purpose and user source inderjeet mani automatic summarization modified text has been replaced with one or more texts particular task has been replaced with particular purpose retrieval ir task of retrieving documents or parts of documents from dataset typically on keyword or natural language queries source din relationship extraction relation extraction task of recognising and labelling semantic relations between entities for natural language text named entity linking nel task of assigning unique identifiers to entities found in natural language text usually from reference knowledge base named entity recognition ner task of recognising and labelling the proper names of entities and their categories for sequences of words in stream of text or speech source din modified denotational name has been replaced with per name notes to entry have been removed data representation of in formal manner suitable for communication interpretation or processing by human beings or computers source iso metadata data that defines and describes other data source din spec structured data data which are organised on applicable set of rules source modified notes to entry have been deleted unstructured data data which are characterised by not having any structure apart from that at the record or file level example free text source modified note to entry has been deleted that record has been replaced with that at the record or file level example has been reworded from an example of unstructured data is free text to free text training data data used to train machine learning model source din labelled data group of data that have been tagged with one or more labels source knowledge maintained processed and interpreted source iso world knowledge commonsense knowledge basic items of knowledge inherent in living organisms as they act learn about themselves and their surrounding and the resulting reactions source iso modified very has been deleted at the beginning not necessarily always witting from early on deeply has been deleted domain knowledge knowledge accumulated in particular domain source modified notes have been deleted knowledge model structured specification of facts that are true for given domain source iso din spec reasoning process by which person or computer performs analysis classification or diagnosis makes assumptions solves problems or draws conclusions source modified inferences has been replaced with conclusions to avoid circular definitions notes to entry have been deleted reasoning reasoning in machine learning model that is restricted to only those entity relationships that conform to the supporting knowledge graph factual framework reasoning cot reasoning reasoning in large language models enhanced by their breaking down complex solving tasks into actions reasoning reasoning in large language models that integrates external tools such as knowledge graph queries to aid in reasoning reasoning that logically combines multiple pieces of from different sources note to entry reasoning is particularly useful for queries where multiple steps are essential to arrive at accurate answers']"
0,48,0_ontology_ontologies_classes_ontological,"['ontology', 'ontologies', 'classes', 'ontological', 'process', 'owl', 'competency', 'creating', 'documentation', 'concepts']","['step ontology documentation description final use case is to document given ontology and to create or enhance the labels and comments for classes and properties', 'ontology documentation in ontological engineering process of documenting given ontology particularly in creating or enhancing labels and comments for classes and properties data access obda in ontological engineering process of accessing database via an ontology which creates view on the data selected solutions and applications question answering description question answering qa is one of the primary applications of large language models llms', 'factuality criterion of trustworthiness that assesses its capability of generating that is rate and on verifiable facts grounding process by which large language model connects its abstract language processing interpretation and generation capabilities to verifiable sources of verification confirmation through the provision of objective evidence that specified requirements have been fulfilled source din modified note to entry has been deleted hallucination factually incorrect generated by an bias atic difference in treatment of certain objects people groups or input data in comparison to others negatively affecting the perception of trustworthiness source din modified negatively affecting the perception of hiness has been added people or groups has been replaced with people groups or input data din spec ontological engineering discipline within science that focuses on the design development and maintenance of ontologies ontology design in ontological engineering process of creating and structuring ontologies competency question generation cq generation in ontological engineering process of creating specific questions designed to assess the capabilities of an in relation to its understanding of particular ontology ontology evaluation in ontological engineering process of assessing the quality and effectiveness of an ontology ontology learning in ontological engineering process of automatically or generating ontologies from unstructured data such as text ontology mapping in ontological engineering process of establishing relationships between concepts in different ontologies note to entry this mapping is indispensable for achieving interoperability between knowledge in domain that rely on differing structures or terminology']"
1,46,1_lehmann_berardinis_andrea_references,"['lehmann', 'berardinis', 'andrea', 'references', 'presutti', 'alejandro', 'stefano', 'carriero', 'jens', 'fabio']","['references jason wei yi tay rishi bommasani colin raffel barret zoph sebastian borgeaud dani tama maarten bosma denny zhou donald metzler ed chi tatsunori hashimoto oriol vinyals percy liang jeff dean and william fedus', 'retrieved from jacopo de berardinis valentina anita carriero nitisha jain nicolas lazzari albert andrea poltronieri and valentina presutti', 'laura weidinger john mellor maribeth rauh conor griffin jonathan uesato huang myra cheng mia glaese borja balle atoosa kasirzadeh zac kenton sasha brown will hawkins tom stepleton ney biles abeba birhane julia haas laura rimell lisa anne hendricks william isaac sean legassick geoffrey irving and iason gabriel']"
2,41,2_kgs_llms_transformer_enhance,"['kgs', 'llms', 'transformer', 'enhance', 'layers', 'verbalising', 'training', 'adaptability', 'knowpat', 'architecture']","['the llm could stay frozen and reuse just the output of the transformer encoder insert knowledge encoding layers in the middle of the transformer layers to adjust the encoding nism enabling the llm to process knowledge from the kg add independent adapters to process ledge', 'answer integrate kgs into llms during by adapting the transformer architecture description these methods use kg knowledge directly during the llm phase by modifying the encoder side of the transformer architecture and improving the training tasks', 'answer integrate kgs into llms by verbalising and including them in llm data description integrating kgs into llms is technique that enhances llms by incorporating verbalised triples from kgs directly into their data']"
3,40,3_knowledge_partitioning_subgraph_multilingual,"['knowledge', 'partitioning', 'subgraph', 'multilingual', 'encoding', 'roadmap', 'aspect', 'unified', 'kgs', 'factuality']","['retrieved november from din spec how do enhance large language models llms by using knowledge graphs kgs', 'large generative graph models', 'how do enhance knowledge graphs kgs by using large language models llms']"
4,38,4_association_computational_linguistics_findings,"['association', 'computational', 'linguistics', 'findings', 'guistics', 'acl', 'conference', 'papers', 'emnlp', 'spain']","['association for computational linguistics toronto canada', 'association for computational linguistics toronto canada', 'in findings of the association for computational linguistics acl']"
5,29,5_wang_zhang_zhao_sun,"['wang', 'zhang', 'zhao', 'sun', 'yang', 'huang', 'peng', 'fan', 'park', 'jiang']","['haiyan zhao hanjie chen fan yang ninghao liu huiqi deng hengyi cai shuaiqiang wang dawei yin and mengnan du', 'haiyan zhao hanjie chen fan yang ninghao liu huiqi deng hengyi cai shuaiqiang wang dawei yin and mengnan du', 'yu sun shuohuan wang shikun feng siyu ding chao pang junyuan shang jiaxiang liu xuyi chen yanbin zhao yuxiang lu weixin liu zhihua wu weibao gong jianzhong liang zhizhou shang peng sun wei liu xuan ouyang dianhai yu hao tian hua wu and haifeng wang']"
6,29,6_trans_acm_heidelberg_nature,"['trans', 'acm', 'heidelberg', 'nature', 'international', 'conference', 'chile', 'avila', 'repository', 'finance']","['springer nature switzerland cham', 'acm trans', 'acm trans']"
7,29,7_factuality_grapheval_factual_consistency,"['factuality', 'grapheval', 'factual', 'consistency', 'inaccuracies', 'validity', 'protocol', 'analyse', 'verifying', 'assessment']","['using frameworks such as grapheval which compares llm outputs with kgs the explainability of llm outputs can also be improved with rable facts see', 'using frameworks such as kontest which tically uses facts in kgs to create consistency tests and check their results factuality assessment can be used to check the logical validity of llm outputs see', 'additionally the use of frameworks like grapheval allows for scalable verification applying facts to atically evaluate and align llm outputs which further mitigates inaccuracies']"
8,28,8_extraction_entities_disambiguation_tasks,"['extraction', 'entities', 'disambiguation', 'tasks', 'mentions', 'coreference', 'texts', 'linked', 'relations', 'step']","['entity linking and disambiguation implement entity linking techniques to connect user queries with entities in kgs ensuring that the accurately identifies and retrieves the necessary mation', 'the implementation steps are as follows first the user question is processed to extract key entities and relationships using entity linking and relationship extraction techniques natural language understanding', 'first the user question is processed to extract key entities and relationships using entity linking and relationship extraction techniques as semantic graph representation of the question natural language understanding']"
9,25,9_explanations_trustworthiness_explainability_assesses,"['explanations', 'trustworthiness', 'explainability', 'assesses', 'interpretability', 'improving', 'educational', 'transparency', 'ability', 'providing']","['explanation generation kgs assist in generating explanations for llm outputs by providing logical path or structure to the answer', 'trustworthiness ability to meet stakeholder expectations in verifiable way source din modified notes to entry have been deleted din spec robustness criterion of trustworthiness that assesses its capability of maintaining its performance level under any circumstances note to entry adverse circumstances that need to be covered by robust can involve the occurrence of unexpected input data', 'interpretability criterion of trustworthiness that assesses its capability of communicating causal hips either contained in data or learned by the model transparency criterion of trustworthiness that assesses its capability of making available appropriate mation about its inherent operation principles to stakeholders explainability criterion of trustworthiness that assesses its capability of expressing its internal mechanisms or process in way that is comprehensible to humans note to entry information on an process covers both its internal mechanisms and important factors ing inputs and training data']"
10,24,10_linguistic_audio_translation_digitaler,"['linguistic', 'audio', 'translation', 'digitaler', 'multilingualism', 'texts', 'ethical', 'corpora', 'textual', 'kgs']","['implementation strategies let us consider kg editing tools and solutions that provide an underlying data model and ui to capture linguistic phenomena and that do cater for multilingualism', 'they structure appropriately but they neglect natural language phenomena particularly multilingualism but also synonymy monitor screen display and ambiguity homonymy jaguar the animal jaguar the car brand', 'thus to be able to establish proper ding of term occurrences in texts with the concept node in the kg requires the kg to be aware of linguistic variations dab acronym digital audio broadcasting full form digitaler digitaler rundfunk radiodiffusion radiodiffusion vysielanie xandir awdjo sonora digital ψηφιακηʆ µραδιοφωνικωʆ εκπομπωʆ trasmissione audio digitale etc']"
11,21,11_tasks_llms_performance_training,"['tasks', 'llms', 'performance', 'training', 'leveraging', 'enterprise', 'classification', 'ranking', 'specificity', 'orientation']","['despite their strong formance on various tasks llms often lack the practical knowledge required for both specific and enterprise applications', 'task artificial intelligence action required to achieve specific goal note to entry actions can be physical or cognitive', 'process of taking model and extending it for task adaption adapting the model for specific new task such as classification or sentiment analysis knowledge enhancement expanding the model knowledge to specialise it for cular domain or enterprise needs instruction tuning teaching the model to follow human instructions using datasets of prompts']"
12,20,12_tail_predicates_entities_relations,"['tail', 'predicates', 'entities', 'relations', 'statements', 'defined', 'linkage', 'openie', 'step', 'extraction']","['extracted triples are typically linked to kg the head and tail entity are linked to que kg entities as explained in the last section and similarly the identified relation is linked to defined kg predicate', 'this involves identifying subject head entity predicate relation and object tail entity', 'in rdf they are defined as subject predicate object while others state them as head entity relation tail entity']"
13,20,13_paths_llms_queries_logical,"['paths', 'llms', 'queries', 'logical', 'chain', 'stepwise', 'associations', 'constraints', 'pog', 'structured']","['chain of thought cot reasoning combined with kgs enables llms to approach reasoning tasks in step structured manner', 'lastly reasoning as seen in frameworks like reasoning and pog directs llm reasoning within predefined kg paths minimizing ir associations and enhancing logical consistency by adhering to factual constraints within the graph structure', 'chain of thought cot reasoning enhanced by kgs cot reasoning when combined with kgs ports structured reasoning process']"
14,19,14_retrieval_databases_retrieves_search,"['retrieval', 'databases', 'retrieves', 'search', 'semantic', 'relational', 'traversal', 'graphdb', 'rdf', 'vectors']","['the retrieval can be done on any source with semantic representation documents with semantic annotations or relational data mapped via data access obda or rdb to rdf mapping language ingesting structured and unstructured source into the graph rag', 'interoperability design the for various graph databases and query languages support integration with external data sources and answer knowledge retrieval mechanisms description retrieval mechanisms involve using kgs or vector databases to enhance the retrieval process in rag kgs provide structured representations of knowledge enabling more precise and contextually aware retrieval', 'conventional generation conventional rag generation that uses unstructured text documents as external knowledge graph generation graph rag generation that integrates knowledge graphs into the rag framework allowing for the retrieval of structured data note to entry the retrieval can be conducted on any source featuring semantic representation for instance ments with semantic annotations or relational data mapped from databases thereby ingesting structured and red source into the graph rag']"
15,18,15_spec_accuracy_programme_kgs,"['spec', 'accuracy', 'programme', 'kgs', 'digital', 'raw', 'llms', 'quality', 'architecture', 'dfg']","['din spec the need for integration enhance accuracy kgs offer rable source of factual which can be used to validate and augment the responses generated by llms', 'additionally in cases where direct comparisons between reference text and output fall short in din spec assessing factual accuracy the output can be converted into meaningful representation to measure alignment with the kg', 'the authors acknowledge the activities of the prects and artificial intelligence dfg bmwk common european language data space digital programme zon europe scilake horizon europe and data spaces support centre digital programme which all contributed to the development of din spec']"
16,17,16_retrieved_liu_zhang_tianyang,"['retrieved', 'liu', 'zhang', 'tianyang', 'qi', 'martha', 'manning', 'lisena', 'ploner', 'raphael']","['retrieved from xiaoze liu feijie wu tianyang xu zhuo chen yichi zhang xiaoqian wang and jing gao', 'retrieved november from bowen yu zhenyu zhang jingyang li haiyang yu tingwen sun jian liu yongbin li and bin wang', 'retrieved november from laura banarescu claire bonial shu cai madalina georgescu kira griffitt ulf hermjakob kevin knight philipp koehn martha palmer and nathan schneider']"
17,15,17_standards_protocols_languages_compliance,"['standards', 'protocols', 'languages', 'compliance', 'specialised', 'kgs', 'frameworks', 'retrieval', 'traversals', 'chuntao']","['standards protocols further reading query languages specialised query languages facilitate efficient and precise retrieval of kg data for purposes', 'standards protocols further reading compliance with data standards ensure the kg adheres to data modelling standards', 'standards protocols further reading compliance with data standards the kg adheres to data modelling standards']"
18,14,18_tokens_modng_masked_mlm,"['tokens', 'modng', 'masked', 'mlm', 'clm', 'language', 'predict', 'entities', 'lating', 'subword']","['in contrast to mlm not only tokens from the text sequence are masked but also entities of the triples that have been added as tokens to the llm data', 'causal language modng clm technique so that the resulting model can be presented with sequence of tokens and learns to predict the next token in the sequence solely on the preceding tokens', 'masked language modng mlm technique so that the resulting model can predict ked token in sequence by considering the context of surrounding tokens']"
19,14,19_answering_graphs_knowledge_commonsenseqa,"['answering', 'graphs', 'knowledge', 'commonsenseqa', 'sql', 'revont', 'dataset', 'guides', 'diverse', 'llms']","['knowledge language models for complex question answering', 'knowledge graphs llms question answering', 'knowledge graphs llms question answering']"
20,14,20_training_symbolic_language_network,"['training', 'symbolic', 'language', 'network', 'transformer', 'processing', 'neurons', 'modelling', 'verbalisation', 'input']","['model architecture foundation for structural design and organisation of components that enables machine learning models to perform tasks din spec transformer model architecture that transforms text into numerals and subsequently into vectors using mechanism to weight the effect of different parts of the input data during processing source modified term transformer neural network has been ted admitted term transformer has become term neural network has been replaced with model architecture that transforms text into numerals and subsequently into vectors has been added layer in hierarchically organised neural network group of artificial neurons whose outputs can connect to neurons in group toward the output of the network but not to neurons in group back toward the input of the network source modified domain indicator artificial intelligence has been ted may has been replaced with can notes to entry have been deleted transformer layer layer that is key component of transformer used in natural language processing note to entry the main parts of transformer layer are mechanism network and in the case of decoder layer an attention module', 'source din modified original note to entry has been replaced with new note to entry that mirrors the syntax of note to entry for subsymbolic machine learning ml process of optimising model parameters through computational techniques such that the model behaviour reflects the data or experience source din supervised machine learning supervised ml machine learning that uses labelled data during training to optimise model parameters source din modified makes only use of labelled data during training has been replaced with uses labelled data during training to optimise model parameters unsupervised machine learning unsupervised ml machine learning that makes only use of unlabelled data during training source din machine learning model ml model mathematical construct that generates an inference or prediction on input data or source din modified example and note to entry deleted language model lm machine learning model that has been trained to represent and produce natural language large language model llm language model that has been trained on extensive text data sets to comprehend and generate natural language text din spec knowledge graph kg graph representation of structured knowledge ing entities and relations between them note to entry knowledge graph can comprise an ontology and assertions to concrete instances of the ontology entities', 'din spec training phase in the life cycle in which the internal parameters of machine learning model are determined or improved in training step in which language model learns language representations by sing big unannotated corpora objective objective that guides the learning process of model from its training data masked language modelling mlm process in in which the model predicts masked token in sequence by ring the context of surrounding tokens causal language modelling clm process in in which the model is presented with sequence of tokens and learns to predict the next token in the sequence solely on the preceding tokens language modelling kmlm masked language modelling in which the token sequence is extended with triples from knowledge graph before masking or where masking choices target tokens relating to entities and relations in knowledge graph finetuning in training step in which the adaptation of machine learning model is continued with training data task adaptation in finetuning process of adapting language model for new specific task example adapting the language model for classification or sentiment analysis tasks knowledge enhancement in finetuning process of expanding language model knowledge to specialise it for particular domain or specific organisation needs instruction finetuning in finetuning process of teaching language model to follow natural language instructions vided through datasets or prompts prompt input provided to an particularly large language model to guide it in generating desired response din spec knowledge graph verbalisation kg verbalisation process of representing knowledge graphs through text thereby transforming structured data into text format from which large language model can process and learn note to entry knowledge graph verbalisation can take place at different stages of the llm life cycle during training or during inference learning']"
21,14,21_bias_biases_attributes_adversarial,"['bias', 'biases', 'attributes', 'adversarial', 'evaluation', 'pipne', 'mitigated', 'lean', 'objectivity', 'attack']","['the first step in setting up such an evaluation pipne is to define kg or extract subgraphs from larger kg covering desirable evaluation bias or biased context', 'this technique can be seen as an adversarial attack because the model is manipulated to check for leveraged bias from the stage that is not mitigated by or other bias gation techniques', 'considerations the bias kg is generated with sensitive attributes that can be potential bias target']"
22,14,22_hallucinations_hallucination_risks_responses,"['hallucinations', 'hallucination', 'risks', 'responses', 'ensuring', 'duce', 'misinterpretation', 'traced', 'understandable', 'mitigate']","['by aligning llm responses with verified facts from kgs we aim to reduce hallucinations and create outputs grounded in rable data', 'thanks to their ability to represent complex relationships and semantics through nodes and relationships kgs are ideal for fact checking helping to reduce the likhood of hallucinations and ensuring that outputs are factually accurate', 'by referencing verified relationships between entities kgs help reduce the occurrence of hallucinations and that responses are factually accurate']"
23,13,23_bases_knowledge_symbolic_integrated,"['bases', 'knowledge', 'symbolic', 'integrated', 'graphs', 'organization', 'wissensgraphen', 'graphes', 'technology', 'models']","['in knowledge engineering and knowledge management annette ten teije johanna siegfried schuh heiner stuckenschmidt mathieu acquin andriy nikolov nathalie and nathalie hernandez eds', 'alleinverkauf durch din media gmbh berlin ics wissensgraphen für sprachmodelle und sprachmodelle für wissensgraphen hybride anwendungen symbolischer und subsymbolischer ki text englisch knowledge graphs for language models and language models for knowledge graphs hybrid applications of symbolic and subsymbolic text in english graphes de connaissances pour les modèles de langage et modèles de langage pour les graphes de applications hybrides de ia symbolique et subsymbolique texte en anglais din spec introduction while language models represent the state of the art in science and technology for variety of language nology tasks there are numerous knowledge bases knowledge graphs and ontologies that contain symbolic knowledge or semantic knowledge in symbolic representation', 'in knowledge engineering and knowledge ment annette ten teije johanna siegfried handschuh heiner stuckenschmidt mathieu acquin andriy nikolov nathalie and nathalie hernandez eds']"
24,13,24_cqs_cq_sparql_competency,"['cqs', 'cq', 'sparql', 'competency', 'questions', 'order', 'stories', 'ontology', 'queries', 'prompt']","['in order to translate natural language questions into sparql queries the llm requires both the ontology and the corresponding cqs to develop queries using the provided identifiers in the ontology', 'step competency question cq generation description once the user stories have been generated whether manually or with the assistance of an llm the subsequent step is to extract competency questions cqs from them', 'alternatively to ascertain whether the cqs are fulfilled by the ontology one may provide the ontology together with the cq in the prompt and request binary answer as to whether the ontology fulfills the cq']"
25,13,25_reasoning_consistency_gap_errors,"['reasoning', 'consistency', 'gap', 'errors', 'combination', 'synthesis', 'distractors', 'evaluations', 'led', 'tracing']","['considerations while kgs are inherently structured to maintain consistency in factual representation llms do not always yield consistent answers especially when queries are rephrased', 'consistency checking in reasoning consistency checking that llms adhere to logical coherence throughout their reasoning process', 'it has limited reasoning capabilities especially with abstract questions that require reasoning inference or the synthesis of new not explicitly stated in the source material']"
26,13,26_graph_statement_entities_semantic,"['graph', 'statement', 'entities', 'semantic', 'knowledge', 'node', 'instances', 'connections', 'concepts', 'relations']","['label identifier associated with node or an edge source modified graph has been deleted entity concrete or abstract thing in the domain under consideration source iso entity instance entity which is part of an abox statement in knowledge graph note to entry usually an entity instance is specific concept or object that also exists in the real world', 'source modified concepts has been replaced with entities relationships has been replaced with relations data related to the ontology has been replaced with assertions to concrete instances of the ontology entities knowledge graph kg knowledge graph that provides on specific subject field knowledge graph kg knowledge graph that covers wide range of topics node vertex in semantic network note to entry node in semantic network typically represents an entity instance or an entity class', 'din spec entity class entity which is part of tbox statement in knowledge graph relation predicate semantic association amongst entities in knowledge graph source modified semantic and in knowledge graph has been added triple statement in knowledge graph in the form that expresses relation ween two entities source din iso modified in knowledge graph has been added resources has been replaced with entities note to entry has been deleted abox statement statement about an entity instance or relation between entity instances in knowledge graph note to entry in abox stands for assertion']"
27,13,27_format_inputs_examples_task,"['format', 'inputs', 'examples', 'task', 'comput', 'prompt', 'outputs', 'symposium', 'computing', 'embedded']","['in such cases it is advisable to either the model to align it with the specific task at hand or at the very least provide examples of the desired output format', 'giving examples using setting including similar inputs and outputs in the requested output format can make the task and the red output format clearer', 'for all sub steps and approaches example inputs and outputs can be included in the prompt to make the task clearer and showcase the desired output format to improve the llm outputs']"
28,12,28_alignment_llms_comparing_metrics,"['alignment', 'llms', 'comparing', 'metrics', 'embedding', 'kgs', 'sync', 'implementations', 'autoregressive', 'transe']","['standards protocols further reading embedding alignment techniques like transe and entity alignment support embedding gnment between llms and kgs', 'alignment metrics measuring alignment between llm and kg representations requires specific metrics', 'answer measuring kg alignment in llm representations description measuring the alignment between llm representations and kgs involves comparing how well the llm output matches the structured knowledge in the kg']"
29,11,29_chat_llm_employed_prompts,"['chat', 'llm', 'employed', 'prompts', 'descriptions', 'cial', 'selecting', 'personalised', 'interactions', 'interviews']","['in interviews with domain experts an llm can be used to describe the scenario and retrieve other pertinent use cases that may be included or explicitly excluded from the modelling domain', 'description llms are used in wide range of business applications such as chatbots virtual assistants or assisted authoring tools enabling companies to provide personalised customer interactions and streamline content creation', 'the llm can then be queried with the subject and relation to predict the object using either pattern predicting the answer using masked guage modelling or predicting the correct statement from item']"
30,11,30_customer_virtual_financial_healthcare,"['customer', 'virtual', 'financial', 'healthcare', 'support', 'chatbots', 'retrieving', 'patient', 'inquiries', 'assistant']","['selected use cases healthcare qa can assist clinicians by interpreting complex medical queries and retrieving research findings from medical kgs', 'customer support enhance virtual assistants ability to resolve customer inquiries by understanding context and retrieving policy or product from or enterprise kgs', 'selected use cases virtual assistants or chatbots for customer support kgs can be used to validate answers before sending them to the user or to enhance the reasoning of the virtual assistant if it uses external tools']"
31,11,31_corr_knowl__,"['corr', 'knowl', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['corr', 'corr', 'corr']"
32,11,32_ontology_models_language_enterprise,"['ontology', 'models', 'language', 'enterprise', 'matching', 'ontological', 'ontochat', 'heritage', 'bertmap', 'owl']","['olala ontology matching with large language models', 'standards protocols further reading hugging face provides an implementation for different peft methods that can also be extended large enterprise language models via ontological reasoning', 'large language models for ontology learning']"
33,11,33_tokens_triples_sentences_interact,"['tokens', 'triples', 'sentences', 'interact', 'labels', 'occur', 'prevents', 'prepends', 'translates', 'trast']","['in trast integrates triples by appending knowledge immediately after the corresponding ities in the text tokens while restricting the triples from influencing text tokens in the sentence', 'to prevent semantic changes to the original meaning of the sentence tokens from the triples are restricted from influencing tokens in the sentence', 'ernie concatenates kg triples with sentences triples sentence and randomly masks tokens from either the sentences or the triples']"
34,10,34_content_aligns_llms_ensuring,"['content', 'aligns', 'llms', 'ensuring', 'trustworthy', 'credibility', 'interpretable', 'trusted', 'traceable', 'justify']","['for example in glam is performed to align llm outputs with entities and relationships ensuring that responses are not only accurate but also interpretable', 'for example frameworks like glam llms to align their outputs with knowledge ensuring that responses are factually accurate and in known data', 'assessing the factuality of llms their ability to generate content that is consistent with accurate that can be verified by rable external sources is critical for business applications to that they generate rable and trustworthy content']"
35,10,35_formats_medical_gnn_edge,"['formats', 'medical', 'gnn', 'edge', 'meaningful', 'translating', 'encode', 'factgraph', 'subgraph', 'summarization']","['for instance graph neural network gnn that encodes edge representations derived from the corresponding entity nodes can be trained on binary classification of factuality or of each encoded edge', 'graph partitioning is employed here to encode the neighbourhood subgraph around each node into textual sentences the graph structures are transformed into format suitable for llm', 'in the generation phase retrieved graph is transformed into formats such as graph languages embeddings or graph neural networks gnn encoding to generate enriched and contextually grounded responses']"
36,10,36_triple_split_extracted_components,"['triple', 'split', 'extracted', 'components', 'texts', 'consecutive', 'constellations', 'duplicates', 'parsers', 'langchain']","['moreover it can be also meaningful to search the kg for existing triples that represent the same as an extracted triple to avoid duplicates', 'this can be performed by checking whether there already is triple with identical components or whether there is triple that is not identical on the but conveys the same semantic', 'considerations when processing longer texts that have to be split because they are too long to fully incorporate them in the prompt it could be that could be split and extraction of this as triple is no longer possible']"
37,9,37_architecture_databases_unstructured_xml,"['architecture', 'databases', 'unstructured', 'xml', 'industrial', 'restructuring', 'schemas', 'tabular', 'flexible', 'consolidated']","['they offer significant advantages over traditional databases semantic richness and interconnection knowledge graphs capture the semantics the meanings and relationships of data not just the data itself', 'flexibility and scalability unlike the rigid schemas of traditional databases kgs have flexible data model that can easily adapt to new types of data and relationships without extensive restructuring', 'data sources can be of different kinds kg data din spec data with graph representation tabular and relational data via obda or data data encoded in extensible markup language xml or darwin mation typing architecture dita unstructured natural language via semantic annotations']"
38,9,38_rag_chunks_accuracy_ches,"['rag', 'chunks', 'accuracy', 'ches', 'feeds', 'retriever', 'storing', 'generator', 'collections', 'chunking']","['by ding the generated output in specific and and documents rag methods improve the quality and accuracy of the generated output', 'there are two types of rag conventional rag has three components knowledge base typically created by chunking text ments transforming them into embeddings and storing them in vector store the retriever ches the vector database for chunks that exhibit high similarity to the query the generator feeds the retrieved chunks alongside the original query to an llm to generate the final response', 'generation rag set of methods that combine language models with external knowledge retrieval to enhance the quality of generated output note to entry rag methods improve the quality and accuracy of the generated output by grounding it in specific and']"
39,8,39_parameters_peft_answers_billions,"['parameters', 'peft', 'answers', 'billions', 'shap', 'pretability', 'feature', 'thorough', 'covering', 'scope']","['this approach that users receive answers that are both broad in scope and deep in detail covering various aspects of query', 'traditional pretability methods such as feature attribution techniques like shap and approaches are putationally intensive and less feasible for models with billions of parameters', 'parameter efficient tuning peft technique to models in which only small number of the model parameters or new small subset of parameters are trained']"
40,8,40_syst_technol_intell_sci,"['syst', 'technol', 'intell', 'sci', 'pmlr', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['intell', 'technol', 'technol']"
41,8,41_toolkengpt_tools_embeddings_augmenting,"['toolkengpt', 'tools', 'embeddings', 'augmenting', 'toolformer', 'tool', 'models', 'lookups', 'layered', 'insights']","['toolkengpt tes embeddings for external tools to as toolkens enabling kg lookups that aid in logical structured reasoning by supplementing the llm outputs with factual data drawn directly from kgs', 'toolkengpt augmenting zen language models with massive tools via tool embeddings', 'toolkengpt augmenting zen language models with massive tools via tool embeddings']"
42,8,42_triples_check_llm_evaluate,"['triples', 'check', 'llm', 'evaluate', 'kg', 'features', 'acceptance', 'reject', 'score', 'comparing']","['furthermore the extracted kg triples can be used to evaluate tasks or features where similarity comparison of the llm output is undesirable', 'kg triples can be used to evaluate the output of an llm by extracting from the output and comparing it with kg to check factuality or knowledge coverage', 'an llm can be prompted to select the most accurate triple from multiple candidates or to accept or reject triple optionally with fidence or acceptance score']"
43,8,43_inconsistencies_concise_efficacy_cues,"['inconsistencies', 'concise', 'efficacy', 'cues', 'consistency', 'usability', 'education', 'tems', 'alignment', 'factuality']","['this approach has shown efficacy in fields like education where models need to generate clear factually supported answers to complex questions', 'by atically llm outputs with kg facts tems like kontest can evaluate the alignment of generated answers with established knowledge identifying logical inconsistencies that may arise during reasoning', 'source modified notes and to entry have been deleted consistency checking in the process of reasoning evaluation of the logical coherence of answers generated by large language models and of these answers alignment with established knowledge represented in ledge graph note to entry consistency checking improves the factuality of answers generated by large language models in cing contradictions between answers and knowledge']"
